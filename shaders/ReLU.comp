#version 450
#extension GL_ARB_separate_shader_objects : enable
#extension GL_EXT_buffer_reference : enable
#extension GL_EXT_buffer_reference_uvec2 : enable
#extension GL_EXT_scalar_block_layout : enable

// Workgroup size (runtime specialization constants)
layout(local_size_x = 256, local_size_y = 1, local_size_z = 1) in;

layout(buffer_reference, std430, scalar) buffer tensor_data {
    float data[];
};

layout(buffer_reference, std430, scalar) buffer tensor_grad {
    float grad[];
};

layout(buffer_reference, std430, scalar) buffer tensor_strides {
    int strides[]; // stride per-dimension
};

layout(buffer_reference, std430, scalar) buffer tensor_shape {
    int dims[]; // size per-dimension
};

struct TensorImpl {
    tensor_data    data;
    tensor_grad    grad;
    tensor_strides strides;
    tensor_shape   shape;
    uint numel;
    uint ndim;
    uint requires_grad;
    uint is_leaf;
};

struct ReLUcontext {
    TensorImpl input_tensor;
    TensorImpl output_tensor;
    uint M, N;
    uint batch_size;         // Total batch size for gradient averaging
    uint mode;              // 0 = in-place, 1 = out-of-place
    uint accumulate_grad;   // 0: overwrite, 1: += for grads
};

layout(buffer_reference, std430, buffer_reference_align = 16) buffer ContextBuffer {
    ReLUcontext ctx;
};

layout(push_constant) uniform PushConstants {
    ContextBuffer context;   // host must match std430 layout
    uvec3 grid_size;         // number of workgroups dispatched (gl_NumWorkGroups)
    uint  kernel_type;       // 0 = forward, 1 = backward (compute dInput)
} push;

// Convert a flat element index (0..numel-1) into the memory offset using the tensor's shape & strides.
// Works for arbitrary ndim.
uint offset_from_flat(const TensorImpl t, uint flat_idx)
{
    uint rem = flat_idx;
    uint off = 0u;

    // For each dimension, compute the coordinate and accumulate offset = coord * stride[dim].
    // coord = rem / product_of_later_dims
    for (uint d = 0u; d < t.ndim; ++d) {
        // compute product of sizes of later dims (denom)
        uint denom = 1u;
        for (uint k = d + 1u; k < t.ndim; ++k) {
            denom *= uint(t.shape.dims[k]);
        }

        uint coord;
        if (denom > 1u) {
            coord = rem / denom;
            rem = rem - coord * denom;
        } else {
            // last dimension or denom == 1
            coord = rem;
            rem = 0u;
        }

        off += coord * uint(t.strides.strides[d]);
    }

    return off;
}

void relu_forward()
{
    ReLUcontext c = push.context.ctx;

    // Global flat element index
    uint gid = gl_GlobalInvocationID.x;
    if (gid >= c.input_tensor.numel) {
        return;
    }

    // Compute memory offsets for input and output for this flat element index
    uint inIdx  = offset_from_flat(c.input_tensor, gid);
    uint outIdx = offset_from_flat(c.output_tensor, gid);

    float inVal = c.input_tensor.data.data[inIdx];
    c.output_tensor.data.data[outIdx] = max(0.0, inVal);
}

void main() {
    // Only forward implemented here. kernel_type can be checked/extended if needed.
    relu_forward();
}