#version 450
#extension GL_ARB_separate_shader_objects : enable
#extension GL_EXT_buffer_reference : enable
#extension GL_EXT_buffer_reference_uvec2 : enable
#extension GL_EXT_scalar_block_layout : enable
#extension GL_KHR_shader_subgroup_basic : enable

// Warp-tiling parameters (matching CUDA kernel structure)
#define BM 128      // Block tile size M
#define BN 128      // Block tile size N
#define BK 8        // Block tile size K
#define WM 64       // Warp tile size M
#define WN 32       // Warp tile size N
#define WNITER 2    // Number of subwarp tiling steps in N dimension
#define TM 8        // Thread tile size M
#define TN 4        // Thread tile size N
#define NUM_THREADS 256

#define WARPSIZE 32

// Calculated constants
const uint WMITER = (WM * WN) / (WARPSIZE * TM * TN * WNITER);  // 2
const uint WSUBM = WM / WMITER;  // 32
const uint WSUBN = WN / WNITER;  // 16

layout(local_size_x = NUM_THREADS, local_size_y = 1, local_size_z = 1) in;

layout(buffer_reference, std430, scalar) buffer tensor_data {
    float data[];
};

layout(buffer_reference, std430, scalar) buffer tensor_grad {
    float grad[];
};

layout(buffer_reference, std430, scalar) buffer tensor_strides {
    int strides[];
};

layout(buffer_reference, std430, scalar) buffer tensor_shape {
    int dims[];
};

struct TensorImpl {
    tensor_data    data;
    tensor_grad    grad;
    tensor_strides strides;
    tensor_shape   shape;
    uint numel, ndim, requires_grad, is_leaf;
};

struct TransposedConv2DContext {
    TensorImpl input_tensor;   // [B, C_in, H_in, W_in]
    TensorImpl weight_tensor;  // [C_in, C_out, KH, KW]  // Note: dims swapped from Conv2d
    TensorImpl bias_tensor;    // [C_out]
    TensorImpl out_tensor;     // [B, C_out, H_out, W_out]
    
    uint batch_size;
    uint in_channels;
    uint out_channels;
    uint in_height;
    uint in_width;
    uint out_height;
    uint out_width;
    uint kernel_h;
    uint kernel_w;
    uint stride_h;
    uint stride_w;
    uint padding_h;
    uint padding_w;
    uint dilation_h;
    uint dilation_w;
    uint output_padding_h;
    uint output_padding_w;
    
    uint use_bias;
    uint accumulate_grad;
    uint kernel_type;
    // GEMM dimensions for TransposedConv2d
    // M = out_height * out_width (output spatial)
    // N = out_channels
    // K = in_channels * in_height * in_width (flattened input)
    uint m, n, k;
};

layout(buffer_reference, std430, buffer_reference_align = 16) buffer ContextBuffer { 
    TransposedConv2DContext ctx; 
};

layout(push_constant) uniform PushConstants {
    ContextBuffer context;
    uvec3 grid_size;
    uint  kernel_type;
} push;

// Shared memory for block tiles (transposed layout for A)
shared float As[BM * BK];  // A tile (transposed: BK x BM)
shared float Bs[BK * BN];  // B tile (BK x BN)

// Helper function to get tensor offset
uint getTensorOffset(const TensorImpl t, uint idx0, uint idx1, uint idx2, uint idx3)
{
    if (t.ndim == 4u) {
        return idx0 * t.strides.strides[0] + 
               idx1 * t.strides.strides[1] + 
               idx2 * t.strides.strides[2] + 
               idx3 * t.strides.strides[3];
    } else if (t.ndim == 3u) {
        return idx0 * t.strides.strides[0] + 
               idx1 * t.strides.strides[1] + 
               idx2 * t.strides.strides[2];
    } else if (t.ndim == 2u) {
        return idx0 * t.strides.strides[0] + 
               idx1 * t.strides.strides[1];
    } else {
        return idx0 * t.strides.strides[0];
    }
}

// Transposed convolution col2im logic
// For TransposedConv2d, we compute which input positions contribute to each output position
// row: output spatial position (0 to M-1) where M = out_height * out_width
// col: input spatial position combined with filter position (0 to K-1)
//      where K = in_channels * in_height * in_width
// Returns weight value for the filter that connects input position to output position
float getTransposedConvWeight(uint batch, uint row, uint col)
{
    TransposedConv2DContext c = push.context.ctx;
    
    // Decompose row into output spatial coordinates
    uint out_w = row % c.out_width;
    uint out_h = row / c.out_width;
    
    // Decompose col into input coordinates
    uint in_w = col % c.in_width;
    uint temp = col / c.in_width;
    uint in_h = temp % c.in_height;
    uint in_c = temp / c.in_height;
    
    // For each output position, we need to find which kernel position connects it to the input
    // out = (in - 1) * stride - 2 * padding + dilation * (kernel - 1) + output_padding + 1
    // Reverse: given out and in, find kernel position
    
    // Calculate which kernel positions can connect this input to this output
    // out_h = in_h * stride_h - padding_h + kh * dilation_h
    // => kh = (out_h + padding_h - in_h * stride_h) / dilation_h
    
    int kh_numerator = int(out_h + c.padding_h) - int(in_h * c.stride_h);
    int kw_numerator = int(out_w + c.padding_w) - int(in_w * c.stride_w);
    
    // Check if this maps to a valid kernel position
    if (kh_numerator < 0 || kw_numerator < 0 ||
        kh_numerator % int(c.dilation_h) != 0 ||
        kw_numerator % int(c.dilation_w) != 0) {
        return 0.0;
    }
    
    int kh = kh_numerator / int(c.dilation_h);
    int kw = kw_numerator / int(c.dilation_w);
    
    // Check kernel bounds
    if (kh < 0 || kh >= int(c.kernel_h) ||
        kw < 0 || kw >= int(c.kernel_w)) {
        return 0.0;
    }
    
    // This function is called to get the "A" matrix value
    // A should be [M x K] where we're computing output positions
    // We need to return the input value at [batch, in_c, in_h, in_w]
    uint idx = getTensorOffset(c.input_tensor, batch, in_c, in_h, in_w);
    return c.input_tensor.data.data[idx];
}

void loadFromGmem(uint M, uint N, uint K, uint bkIdx, uint batch,
                  uint innerRowA, uint innerColA, uint innerRowB, uint innerColB)
{
    TransposedConv2DContext c = push.context.ctx;
    const uint rowStrideA = (NUM_THREADS * 4) / BK;
    const uint rowStrideB = NUM_THREADS / (BN / 4);
    
    // Load A matrix - Input reshaped
    // A is [M x K] where M = out_h * out_w, K = in_c * in_h * in_w
    // For transposed conv, we load input and implicitly handle the kernel mapping
    for (uint offset = 0; offset < BM; offset += rowStrideA) {
        uint globalRow = gl_WorkGroupID.y * BM + innerRowA + offset;
        uint globalCol = bkIdx + innerColA * 4;
        
        // Vectorized load (4 floats)
        vec4 tmp = vec4(0.0);
        if (globalRow < M && globalCol < K) {
            tmp.x = getTransposedConvWeight(batch, globalRow, globalCol);
            if (globalCol + 1 < K) tmp.y = getTransposedConvWeight(batch, globalRow, globalCol + 1);
            if (globalCol + 2 < K) tmp.z = getTransposedConvWeight(batch, globalRow, globalCol + 2);
            if (globalCol + 3 < K) tmp.w = getTransposedConvWeight(batch, globalRow, globalCol + 3);
        }
        
        // Store transposed: As[col][row] = A[row][col]
        As[(innerColA * 4 + 0) * BM + innerRowA + offset] = tmp.x;
        As[(innerColA * 4 + 1) * BM + innerRowA + offset] = tmp.y;
        As[(innerColA * 4 + 2) * BM + innerRowA + offset] = tmp.z;
        As[(innerColA * 4 + 3) * BM + innerRowA + offset] = tmp.w;
    }
    
    // Load B matrix (Weight for transposed convolution)
    // Weight is [C_in, C_out, KH, KW] - NOT contiguous for our access pattern!
    // We need B as [K x N] where K = in_c * in_h * in_w, N = C_out
    // We have to brute force this with scalar loads
    
    // Each thread loads one element at a time
    const uint totalElements = BK * BN;
    const uint elemsPerThread = (totalElements + NUM_THREADS - 1) / NUM_THREADS;
    
    for (uint e = 0; e < elemsPerThread; ++e) {
        uint linearIdx = gl_LocalInvocationID.x * elemsPerThread + e;
        if (linearIdx >= totalElements) break;
        
        // Decompose linear index into [k, n] for shared memory
        uint smem_k = linearIdx / BN;
        uint smem_n = linearIdx % BN;
        
        // Calculate global indices
        uint globalK = bkIdx + smem_k;      // K dimension: in_c * in_h * in_w
        uint globalN = gl_WorkGroupID.x * BN + smem_n;  // N dimension: out_channels
        
        float value = 0.0;
        
        if (globalK < K && globalN < N) {
            // Decompose K index into input spatial coordinates
            uint in_w = globalK % c.in_width;
            uint temp_k = globalK / c.in_width;
            uint in_h = temp_k % c.in_height;
            uint in_c = temp_k / c.in_height;
            
            uint out_c = globalN;
            
            // For transposed conv, we need to sum over all kernel positions
            // that could connect input[in_c, in_h, in_w] to any output[out_c, *, *]
            // Weight layout: [in_c, out_c, kh, kw]
            // We sum over all (kh, kw) that are valid for this input position
            
            for (uint kh = 0; kh < c.kernel_h; ++kh) {
                for (uint kw = 0; kw < c.kernel_w; ++kw) {
                    // Calculate which output positions this kernel tap contributes to
                    // out_h = in_h * stride_h - padding_h + kh * dilation_h
                    // out_w = in_w * stride_w - padding_w + kw * dilation_w
                    
                    int out_h = int(in_h * c.stride_h) - int(c.padding_h) + int(kh * c.dilation_h);
                    int out_w = int(in_w * c.stride_w) - int(c.padding_w) + int(kw * c.dilation_w);
                    
                    // Check if this output position is valid
                    if (out_h >= 0 && out_h < int(c.out_height) &&
                        out_w >= 0 && out_w < int(c.out_width)) {
                        
                        // Load weight: weight[in_c, out_c, kh, kw]
                        uint weightIdx = getTensorOffset(c.weight_tensor, in_c, out_c, kh, kw);
                        value += c.weight_tensor.data.data[weightIdx];
                    }
                }
            }
        }
        
        // Store in shared memory Bs[k][n]
        Bs[smem_k * BN + smem_n] = value;
    }
}

void processFromSmem(inout float threadResults[WMITER * TM * WNITER * TN],
                     uint warpRow, uint warpCol,
                     uint threadRowInWarp, uint threadColInWarp)
{
    float regM[WMITER * TM];
    float regN[WNITER * TN];
    
    for (uint dotIdx = 0; dotIdx < BK; ++dotIdx) {
        // Populate registers for whole warptile
        for (uint wSubRowIdx = 0; wSubRowIdx < WMITER; ++wSubRowIdx) {
            for (uint i = 0; i < TM; ++i) {
                regM[wSubRowIdx * TM + i] = 
                    As[(dotIdx * BM) + warpRow * WM + wSubRowIdx * WSUBM + 
                       threadRowInWarp * TM + i];
            }
        }
        
        for (uint wSubColIdx = 0; wSubColIdx < WNITER; ++wSubColIdx) {
            for (uint i = 0; i < TN; ++i) {
                regN[wSubColIdx * TN + i] = 
                    Bs[(dotIdx * BN) + warpCol * WN + wSubColIdx * WSUBN + 
                       threadColInWarp * TN + i];
            }
        }
        
        // Execute warptile matmul
        for (uint wSubRowIdx = 0; wSubRowIdx < WMITER; ++wSubRowIdx) {
            for (uint wSubColIdx = 0; wSubColIdx < WNITER; ++wSubColIdx) {
                for (uint resIdxM = 0; resIdxM < TM; ++resIdxM) {
                    for (uint resIdxN = 0; resIdxN < TN; ++resIdxN) {
                        threadResults[(wSubRowIdx * TM + resIdxM) * (WNITER * TN) +
                                     (wSubColIdx * TN) + resIdxN] +=
                            regM[wSubRowIdx * TM + resIdxM] * 
                            regN[wSubColIdx * TN + resIdxN];
                    }
                }
            }
        }
    }
}

void forward(uint B, uint M, uint N, uint K)
{
    TransposedConv2DContext c = push.context.ctx;
    
    const uint cRow = gl_WorkGroupID.y;
    const uint cCol = gl_WorkGroupID.x;
    const uint batch = gl_WorkGroupID.z;
    
    // Warp placement in threadblock tile
    const uint warpIdx = gl_LocalInvocationID.x / WARPSIZE;
    const uint warpCol = warpIdx % (BN / WN);
    const uint warpRow = warpIdx / (BN / WN);
    
    // Thread placement in warp subtile
    const uint threadIdxInWarp = gl_LocalInvocationID.x % WARPSIZE;
    const uint threadColInWarp = threadIdxInWarp % (WSUBN / TN);
    const uint threadRowInWarp = threadIdxInWarp / (WSUBN / TN);
    
    // Calculate indices for loading into shared memory
    const uint innerRowA = gl_LocalInvocationID.x / (BK / 4);
    const uint innerColA = gl_LocalInvocationID.x % (BK / 4);
    const uint innerRowB = gl_LocalInvocationID.x / (BN / 4);
    const uint innerColB = gl_LocalInvocationID.x % (BN / 4);
    
    // Allocate thread-local cache for results
    float threadResults[WMITER * TM * WNITER * TN];
    for (uint i = 0; i < WMITER * TM * WNITER * TN; ++i) {
        threadResults[i] = 0.0;
    }
    
    // Outer-most loop over block tiles
    for (uint bkIdx = 0; bkIdx < K; bkIdx += BK) {
        loadFromGmem(M, N, K, bkIdx, batch, innerRowA, innerColA, innerRowB, innerColB);
        barrier();
        
        processFromSmem(threadResults, warpRow, warpCol, threadRowInWarp, threadColInWarp);
        
        barrier();
    }
    
    // Write out the results
    for (uint wSubRowIdx = 0; wSubRowIdx < WMITER; ++wSubRowIdx) {
        for (uint wSubColIdx = 0; wSubColIdx < WNITER; ++wSubColIdx) {
            for (uint resIdxM = 0; resIdxM < TM; resIdxM += 1) {
                for (uint resIdxN = 0; resIdxN < TN; resIdxN += 1) {
                    uint globalRow = cRow * BM + warpRow * WM + wSubRowIdx * WSUBM + 
                                    threadRowInWarp * TM + resIdxM;
                    uint globalCol = cCol * BN + warpCol * WN + wSubColIdx * WSUBN + 
                                    threadColInWarp * TN + resIdxN;
                    
                    // Bounds check
                    if (globalRow < M && globalCol < N) {
                        uint i = (wSubRowIdx * TM + resIdxM) * (WNITER * TN) +
                                wSubColIdx * TN + resIdxN;
                        float result = threadResults[i];
                        
                        // Decompose globalRow back into spatial coordinates
                        uint out_w = globalRow % c.out_width;
                        uint out_h = globalRow / c.out_width;
                        uint out_c = globalCol;
                        
                        // Add bias if enabled
                        if (c.use_bias != 0u) {
                            uint biasIdx = getTensorOffset(c.bias_tensor, out_c, 0, 0, 0);
                            result += c.bias_tensor.data.data[biasIdx];
                        }

                        // Write output [batch, out_c, out_h, out_w]
                        uint idxOut = getTensorOffset(c.out_tensor, batch, out_c, out_h, out_w);
                        c.out_tensor.data.data[idxOut] = result;
                    }
                }
            }
        }
    }
}

void main()
{
    TransposedConv2DContext c = push.context.ctx;
    uint B = max(c.batch_size, 1u);
    uint M = c.m;  // out_height * out_width
    uint N = c.n;  // out_channels
    uint K = c.k;  // in_channels * in_height * in_width
    
    forward(B, M, N, K);
}