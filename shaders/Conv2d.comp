#version 460
#extension GL_ARB_separate_shader_objects : enable
#extension GL_EXT_buffer_reference : enable
#extension GL_EXT_buffer_reference_uvec2 : enable
#extension GL_EXT_scalar_block_layout : enable
#extension GL_KHR_shader_subgroup_basic : enable
#extension GL_KHR_shader_subgroup_shuffle : enable
#extension GL_KHR_shader_subgroup_arithmetic : enable
#extension GL_EXT_control_flow_attributes : enable

// Workgroup configuration - matches CUDA's thread block
#define TILE 16
#define OUT_TILE_X 2
#define OUT_TILE_Y 2
#define THREADS_X (TILE / OUT_TILE_X)
#define THREADS_Y (TILE / OUT_TILE_Y)

// Conservative maximums for shared memory allocation
#define MAX_KERNEL_SIZE 10  // Supports up to 10x10 kernels
#define MAX_PAD 3           // Maximum padding supported
#define MAX_DILATION 3      // Maximum dilation supported
#define MAX_STRIDE 4        // Maximum stride supported

// Calculate maximum shared memory dimensions needed
// Worst case: large stride, large kernel with dilation, and padding
#define SHARED_H_MAX (TILE * MAX_STRIDE + (MAX_KERNEL_SIZE - 1) * MAX_DILATION + 1 + MAX_PAD * 2)
#define SHARED_W_MAX (TILE * MAX_STRIDE + (MAX_KERNEL_SIZE - 1) * MAX_DILATION + 1 + MAX_PAD * 2)
#define SHARED_W_PAD (SHARED_W_MAX + 1)  // +1 to avoid bank conflicts

layout(local_size_x = THREADS_X, local_size_y = THREADS_Y, local_size_z = 1) in;

layout(buffer_reference, std430, scalar) buffer tensor_data {
    float data[];
};

layout(buffer_reference, std430, scalar) buffer tensor_grad {
    float grad[];
};

layout(buffer_reference, std430, scalar) buffer tensor_strides {
    int strides[];
};

layout(buffer_reference, std430, scalar) buffer tensor_shape {
    int dims[];
};

struct TensorImpl {
    tensor_data data;
    tensor_grad grad;
    tensor_strides strides;
    tensor_shape shape;
    uint numel, ndim, requires_grad, is_leaf;
};

struct Conv2dContext {
    TensorImpl input_tensor;  // [N, C_in, H_in, W_in]
    TensorImpl weight_tensor; // [C_out, C_in, K_h, K_w]
    TensorImpl bias_tensor;   // [C_out]
    TensorImpl out_tensor;    // [N, C_out, H_out, W_out]
    uint stride_h;
    uint stride_w;
    uint pad_h;
    uint pad_w;
    uint dilation_h;
    uint dilation_w;
    uint kernel_h;            // Dynamic kernel height
    uint kernel_w;            // Dynamic kernel width
    uint groups;
    uint accumulate_grad;
    uint kernel_type; // doesnt do anything in forward pass
};

layout(buffer_reference, std430, scalar) buffer ContextBuffer {
    Conv2dContext ctx;
};

layout(push_constant) uniform PushConstants {
    ContextBuffer context;
    uvec3 grid_size;
    uint kernel_type;
} push;

// Shared memory tile - allocated for maximum size
shared float sh[SHARED_H_MAX][SHARED_W_PAD];

// Helper function to load 4 floats (vectorized load)
void load4_shared(int sh_x, int sh_y, int global_offset, int valid_count, tensor_data input_data) {
    vec4 v = vec4(0.0);
    
    if (valid_count >= 4) {
        // Load all 4 values at once
        v.x = input_data.data[global_offset + 0];
        v.y = input_data.data[global_offset + 1];
        v.z = input_data.data[global_offset + 2];
        v.w = input_data.data[global_offset + 3];
    } else if (valid_count > 0) {
        // Partial load for boundary cases
        for (int i = 0; i < valid_count; ++i) {
            v[i] = input_data.data[global_offset + i];
        }
    }
    
    // Store to shared memory
    sh[sh_y][sh_x + 0] = v.x;
    sh[sh_y][sh_x + 1] = v.y;
    sh[sh_y][sh_x + 2] = v.z;
    sh[sh_y][sh_x + 3] = v.w;
}

void main() {
    Conv2dContext ctx = push.context.ctx;
    
    // Extract dimensions from tensor shapes
    int B = int(ctx.input_tensor.shape.dims[0]);   // batch size
    int C_in = int(ctx.input_tensor.shape.dims[1]); // input channels
    int H_in = int(ctx.input_tensor.shape.dims[2]); // input height
    int W_in = int(ctx.input_tensor.shape.dims[3]); // input width
    int C_out = int(ctx.weight_tensor.shape.dims[0]); // output channels (filters)
    int H_out = int(ctx.out_tensor.shape.dims[2]); // output height
    int W_out = int(ctx.out_tensor.shape.dims[3]); // output width
    
    // Extract convolution parameters
    int stride_h = int(ctx.stride_h);
    int stride_w = int(ctx.stride_w);
    int pad_h = int(ctx.pad_h);
    int pad_w = int(ctx.pad_w);
    int dilation_h = int(ctx.dilation_h);
    int dilation_w = int(ctx.dilation_w);
    int K_h = int(ctx.kernel_h);  // Dynamic kernel dimensions
    int K_w = int(ctx.kernel_w);
    
    // Calculate effective kernel size with dilation
    int kernel_h_eff = (K_h - 1) * dilation_h + 1;
    int kernel_w_eff = (K_w - 1) * dilation_w + 1;
    
    // Calculate actual shared memory tile size needed for this convolution
    int shared_h = TILE * stride_h + kernel_h_eff - stride_h + pad_h * 2;
    int shared_w = TILE * stride_w + kernel_w_eff - stride_w + pad_w * 2;
    
    // Clamp to maximum allocated size
    shared_h = min(shared_h, SHARED_H_MAX);
    shared_w = min(shared_w, SHARED_W_MAX);
    
    // Block and thread indices
    int block_out_x = int(gl_WorkGroupID.x) * TILE;
    int block_out_y = int(gl_WorkGroupID.y) * TILE;
    int tx = int(gl_LocalInvocationID.x);
    int ty = int(gl_LocalInvocationID.y);
    
    // Output pixel coordinates for this thread
    int out_x0 = block_out_x + tx * OUT_TILE_X;
    int out_y0 = block_out_y + ty * OUT_TILE_Y;
    
    // Batch and filter index from z dimension
    int b = int(gl_WorkGroupID.z) / C_out;
    int f = int(gl_WorkGroupID.z) % C_out;
    
    if (b >= B) return;
    
    // Base offsets for this batch
    int input_b_offset = b * C_in * H_in * W_in;
    int output_b_offset = b * C_out * H_out * W_out;
    
    // Accumulator for output tile (each thread computes OUT_TILE_Y x OUT_TILE_X outputs)
    float acc[OUT_TILE_Y][OUT_TILE_X];
    for (int oy = 0; oy < OUT_TILE_Y; ++oy) {
        for (int ox = 0; ox < OUT_TILE_X; ++ox) {
            acc[oy][ox] = 0.0;
        }
    }
    
    // Loop over input channels
    for (int c = 0; c < C_in; ++c) {
        // Calculate input region needed for this output tile
        int in_y_start = block_out_y * stride_h - pad_h;
        int in_x_start = block_out_x * stride_w - pad_w;
        
        // Load data into shared memory (vectorized loads)
        for (int sy = ty; sy < shared_h; sy += THREADS_Y) {
            int in_y = in_y_start + sy;
            
            for (int sx = tx * 4; sx < shared_w; sx += THREADS_X * 4) {
                int in_x = in_x_start + sx;
                
                // Determine how many valid elements we can load
                int valid = 0;
                if (in_y >= 0 && in_y < H_in) {
                    if (in_x >= 0 && in_x + 3 < W_in) {
                        valid = 4;  // All 4 elements valid
                    } else if (in_x >= 0 && in_x < W_in) {
                        valid = W_in - in_x;  // Partial validity
                    }
                }
                
                if (valid > 0) {
                    int global_offset = input_b_offset + c * H_in * W_in + in_y * W_in + in_x;
                    load4_shared(sx, sy, global_offset, valid, ctx.input_tensor.data);
                } else {
                    // Zero padding for out-of-bounds regions
                    sh[sy][sx + 0] = 0.0;
                    sh[sy][sx + 1] = 0.0;
                    sh[sy][sx + 2] = 0.0;
                    sh[sy][sx + 3] = 0.0;
                }
            }
        }
        
        barrier();
        
        // Compute convolution with dynamic kernel size
        int kbase = (f * C_in + c) * K_h * K_w;
        
        for (int oy = 0; oy < OUT_TILE_Y; ++oy) {
            for (int ox = 0; ox < OUT_TILE_X; ++ox) {
                int out_y = out_y0 + oy;
                int out_x = out_x0 + ox;
                
                if (out_y >= H_out || out_x >= W_out) continue;
                
                // Calculate position in shared memory
                int sy_base = (out_y - block_out_y) * stride_h + pad_h;
                int sx_base = (out_x - block_out_x) * stride_w + pad_w;
                
                float sum = 0.0;
                
                // Dynamic kernel convolution
                for (int ky = 0; ky < K_h; ++ky) {
                    for (int kx = 0; kx < K_w; ++kx) {
                        int widx = kbase + ky * K_w + kx;
                        int sy = sy_base + ky * dilation_h;
                        int sx = sx_base + kx * dilation_w;
                        sum += sh[sy][sx] * ctx.weight_tensor.data.data[widx];
                    }
                }
                
                acc[oy][ox] += sum;
            }
        }
        
        barrier();
    }
    
    // Write outputs with bias
    for (int oy = 0; oy < OUT_TILE_Y; ++oy) {
        int out_y = out_y0 + oy;
        if (out_y < 0 || out_y >= H_out) continue;
        
        for (int ox = 0; ox < OUT_TILE_X; ++ox) {
            int out_x = out_x0 + ox;
            if (out_x < 0 || out_x >= W_out) continue;
            
            int out_idx = output_b_offset + f * (H_out * W_out) + out_y * W_out + out_x;
            
            // Add bias and write result
            float result = acc[oy][ox];
            result += ctx.bias_tensor.data.data[f];
            ctx.out_tensor.data.data[out_idx] = result;
        }
    }
}