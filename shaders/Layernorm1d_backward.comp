#version 460
#extension GL_ARB_separate_shader_objects : enable
#extension GL_EXT_buffer_reference : enable
#extension GL_EXT_buffer_reference_uvec2 : enable
#extension GL_EXT_scalar_block_layout : enable
#extension GL_EXT_shader_atomic_float : enable
#extension GL_KHR_shader_subgroup_basic : enable
#extension GL_KHR_shader_subgroup_shuffle : enable
#extension GL_KHR_shader_subgroup_arithmetic : enable
#extension GL_KHR_shader_subgroup_shuffle_relative : enable

#define MAX_WG 256u
#define WARP_SIZE 32u
layout(local_size_x = 256, local_size_y = 1, local_size_z = 1) in;

layout(buffer_reference, std430, scalar) buffer tensor_data { float data[]; };
layout(buffer_reference, std430, scalar) buffer tensor_grad { float grad[]; };
layout(buffer_reference, std430, scalar) buffer tensor_strides { int strides[]; };
layout(buffer_reference, std430, scalar) buffer tensor_shape { int dims[]; };

struct TensorImpl {
    tensor_data data;
    tensor_grad grad;
    tensor_strides strides;
    tensor_shape shape;
    uint numel, ndim, requires_grad, is_leaf;
};

struct LayerNorm1DContext {
    TensorImpl input_tensor;    // [B, M, N]
    TensorImpl weight_tensor;   // [N]
    TensorImpl bias_tensor;     // [N]
    TensorImpl out_tensor;      // [B, M, N]  (out.grad holds upstream grads)
    TensorImpl save_mean;       // [B, M]
    TensorImpl save_rstd;       // [B, M]
    uint mode;                  // 0 (training)
    uint normalized_size;       // N
    uint batch_stride;          // M * N
    uint accumulate_grad;
    float eps;
};

layout(buffer_reference, std430, scalar) buffer ContextBuffer { LayerNorm1DContext ctx; };

layout(push_constant) uniform PushConstants {
    ContextBuffer context;
    uvec3 grid_size;
    uint kernel_type;
} push;

shared float shmem[MAX_WG];
shared float shmem2[MAX_WG];

const uint VEC4 = 4u;
const uint VEC4S_PER_THREAD = 2u;            // tuneable: 1..4
const uint FLOATS_PER_THREAD = VEC4 * VEC4S_PER_THREAD;
const uint NUM_WARPS = MAX_WG / WARP_SIZE;

// contiguous vec4 load (handles tail)
vec4 loadVec4Contig(tensor_data buf, uint base_idx, uint remaining) {
    vec4 r = vec4(0.0);
    if (remaining >= 4u) {
        r.x = buf.data[base_idx + 0u];
        r.y = buf.data[base_idx + 1u];
        r.z = buf.data[base_idx + 2u];
        r.w = buf.data[base_idx + 3u];
    } else {
        if (remaining >= 1u) r.x = buf.data[base_idx + 0u];
        if (remaining >= 2u) r.y = buf.data[base_idx + 1u];
        if (remaining >= 3u) r.z = buf.data[base_idx + 2u];
    }
    return r;
}

// store vec4 to input.grad with optional atomic add
void storeVec4Grad(tensor_grad buf, uint base_idx, vec4 v, uint remaining, bool accumulate) {
    if (accumulate) {
        if (remaining >= 1u) atomicAdd(buf.grad[base_idx + 0u], v.x);
        if (remaining >= 2u) atomicAdd(buf.grad[base_idx + 1u], v.y);
        if (remaining >= 3u) atomicAdd(buf.grad[base_idx + 2u], v.z);
        if (remaining >= 4u) atomicAdd(buf.grad[base_idx + 3u], v.w);
    } else {
        if (remaining >= 1u) buf.grad[base_idx + 0u] = v.x;
        if (remaining >= 2u) buf.grad[base_idx + 1u] = v.y;
        if (remaining >= 3u) buf.grad[base_idx + 2u] = v.z;
        if (remaining >= 4u) buf.grad[base_idx + 3u] = v.w;
    }
}

// atomic add for param grads (float buffer)
void atomicAddParam(tensor_grad buf, uint idx, float v) {
    atomicAdd(buf.grad[idx], v);
}

// block-level two-value reduction using subgroup ops + shared memory
void blockReduceSum2(float val1, float val2, out float result1, out float result2) {
    uint tid = gl_LocalInvocationID.x;
    uint warp_id = tid / WARP_SIZE;
    uint lane_id = tid % WARP_SIZE;

    // warp-level
    val1 = subgroupAdd(val1);
    val2 = subgroupAdd(val2);

    if (subgroupElect()) {
        shmem[warp_id] = val1;
        shmem2[warp_id] = val2;
    }
    barrier();

    // finish in first warp
    if (warp_id == 0u) {
        float a = (lane_id < NUM_WARPS) ? shmem[lane_id] : 0.0;
        float b = (lane_id < NUM_WARPS) ? shmem2[lane_id] : 0.0;
        a = subgroupAdd(a);
        b = subgroupAdd(b);
        // broadcast to all lanes in first warp (only lanes in warp 0 will get non-zero)
        result1 = a;
        result2 = b;
    } else {
        result1 = 0.0;
        result2 = 0.0;
    }
}

void backward_kernel() {
    LayerNorm1DContext ctx = push.context.ctx;
    uint sample_idx = gl_WorkGroupID.x; // flattened [B, M]
    uint tid = gl_LocalInvocationID.x;
    uint lane = tid % WARP_SIZE;
    uint warp_id = tid / WARP_SIZE;

    uint N = ctx.normalized_size;
    uint base_idx = sample_idx * N;

    // saved stats
    float mean = ctx.save_mean.data.data[sample_idx];
    float rstd = ctx.save_rstd.data.data[sample_idx];

    // tiling params
    uint warp_tile_floats = WARP_SIZE * FLOATS_PER_THREAD;
    uint num_warps = NUM_WARPS;
    uint grid_stride = num_warps * warp_tile_floats;

    // per-thread accumulators for the two reductions used in input grad
    float t_sum_grad_norm = 0.0;        // sum of grad_normalized
    float t_sum_grad_norm_times_norm = 0.0; // sum of grad_normalized * normalized

    // Phase 1: per-element work: compute normalized, grad_w, grad_b, grad_normalized
    // Vectorized over vec4s
    for (uint tile_base = warp_id * warp_tile_floats; tile_base < N; tile_base += grid_stride) {
        uint thread_start = tile_base + lane * FLOATS_PER_THREAD;
        for (uint v = 0u; v < VEC4S_PER_THREAD; ++v) {
            uint idx = thread_start + v * VEC4;
            if (idx >= N) break;
            uint remaining = min(VEC4, N - idx);

            // load upstream grad (out.grad), input x, weight
            vec4 grad_out_v = loadVec4Contig(ctx.out_tensor.grad, base_idx + idx, remaining);
            vec4 x_v        = loadVec4Contig(ctx.input_tensor.data, base_idx + idx, remaining);
            vec4 w_v        = loadVec4Contig(ctx.weight_tensor.data, idx, remaining);

            // normalized = (x - mean) * rstd
            vec4 norm_v = (x_v - vec4(mean)) * vec4(rstd);

            // grad_w = grad_out * normalized  (per-feature)
            vec4 grad_w_v = grad_out_v * norm_v;
            // grad_b = grad_out
            vec4 grad_b_v = grad_out_v;

            // write parameter grads atomically (one atomic per scalar component)
            // atomicAdd is required because many samples write to same param elements
            // note: weight_tensor.grad and bias_tensor.grad indexed by feature idx (relative to N)
            for (uint c = 0u; c < remaining; ++c) {
                uint feature_idx = idx + c;
                float gw = (c==0u) ? grad_w_v.x : (c==1u) ? grad_w_v.y : (c==2u) ? grad_w_v.z : grad_w_v.w;
                float gb = (c==0u) ? grad_b_v.x : (c==1u) ? grad_b_v.y : (c==2u) ? grad_b_v.z : grad_b_v.w;
                atomicAddParam(ctx.weight_tensor.grad, feature_idx, gw);
                atomicAddParam(ctx.bias_tensor.grad, feature_idx, gb);
            }

            // grad_normalized = grad_out * weight
            vec4 grad_norm_v = grad_out_v * w_v;

            // accumulate reductions needed for input gradient
            // sum of grad_normalized components
            t_sum_grad_norm += (grad_norm_v.x + grad_norm_v.y + grad_norm_v.z + grad_norm_v.w);
            // dot(grad_normalized, normalized)
            t_sum_grad_norm_times_norm += dot(grad_norm_v, norm_v);
        }
    }

    // reduce across block to get totals
    float total_grad_norm = 0.0;
    float total_grad_norm_times_norm = 0.0;
    blockReduceSum2(t_sum_grad_norm, t_sum_grad_norm_times_norm,
                    total_grad_norm, total_grad_norm_times_norm);

    barrier();

    // only warp 0 has non-zero totals returned in its lanes; lane 0 of workgroup (tid==0) writes normalized means to shared
    if (tid == 0u) {
        // totals in first warp's lane0 are in total_grad_norm/total_grad_norm_times_norm
        float mean_g = total_grad_norm / float(N);
        float mean_gn = total_grad_norm_times_norm / float(N);
        shmem[0] = mean_g;
        shmem[1] = mean_gn;
    }
    barrier();

    float mean_grad_norm = shmem[0];
    float mean_grad_norm_times_norm = shmem[1];

    // Phase 2: compute input gradients and zero upstream grads
    for (uint tile_base = warp_id * warp_tile_floats; tile_base < N; tile_base += grid_stride) {
        uint thread_start = tile_base + lane * FLOATS_PER_THREAD;
        for (uint v = 0u; v < VEC4S_PER_THREAD; ++v) {
            uint idx = thread_start + v * VEC4;
            if (idx >= N) break;
            uint remaining = min(VEC4, N - idx);

            vec4 grad_out_v = loadVec4Contig(ctx.out_tensor.grad, base_idx + idx, remaining);
            vec4 x_v        = loadVec4Contig(ctx.input_tensor.data, base_idx + idx, remaining);
            vec4 w_v        = loadVec4Contig(ctx.weight_tensor.data, idx, remaining);

            vec4 norm_v = (x_v - vec4(mean)) * vec4(rstd);
            vec4 grad_norm_v = grad_out_v * w_v;

            // grad_input = (grad_normalized - mean_grad_norm - normalized * mean_grad_norm_times_norm) * rstd
            vec4 g = (grad_norm_v - vec4(mean_grad_norm) - norm_v * vec4(mean_grad_norm_times_norm)) * vec4(rstd);

            // write input grad (atomic or store)
            storeVec4Grad(ctx.input_tensor.grad, base_idx + idx, g, remaining, ctx.accumulate_grad == 1u);

            // zero out upstream grad
            // atomic not needed for zeroing since this workgroup owns sample's upstream grads, but use store
            if (remaining >= 1u) ctx.out_tensor.grad.grad[base_idx + idx + 0u] = 0.0;
            if (remaining >= 2u) ctx.out_tensor.grad.grad[base_idx + idx + 1u] = 0.0;
            if (remaining >= 3u) ctx.out_tensor.grad.grad[base_idx + idx + 2u] = 0.0;
            if (remaining >= 4u) ctx.out_tensor.grad.grad[base_idx + idx + 3u] = 0.0;
        }
    }
}

void main() { backward_kernel(); }
