#version 460
#extension GL_ARB_separate_shader_objects : enable
#extension GL_EXT_buffer_reference : enable
#extension GL_EXT_buffer_reference_uvec2 : enable
#extension GL_EXT_scalar_block_layout : enable
#extension GL_EXT_shader_atomic_float : enable
#extension GL_KHR_shader_subgroup_basic : enable
#extension GL_KHR_shader_subgroup_shuffle : enable
#extension GL_KHR_shader_subgroup_arithmetic : enable
#extension GL_KHR_shader_subgroup_shuffle_relative : enable

#define MAX_WG 256u
#define WARP_SIZE 32u
layout(local_size_x = 256, local_size_y = 1, local_size_z = 1) in;

layout(buffer_reference, std430, scalar) buffer tensor_data {
    float data[];
};
layout(buffer_reference, std430, scalar) buffer tensor_grad {
    float grad[];
};
layout(buffer_reference, std430, scalar) buffer tensor_strides {
    int strides[];
};
layout(buffer_reference, std430, scalar) buffer tensor_shape {
    int dims[];
};

struct TensorImpl {
    tensor_data data;
    tensor_grad grad;
    tensor_strides strides;
    tensor_shape shape;
    uint numel, ndim, requires_grad, is_leaf;
};

struct BatchNorm2DContext {
    TensorImpl input_tensor;    // [B, C, H, W]
    TensorImpl weight_tensor;   // [C]
    TensorImpl bias_tensor;     // [C]
    TensorImpl running_mean;    // [C]
    TensorImpl running_var;     // [C]
    TensorImpl out_tensor;      // [B, C, H, W]
    TensorImpl save_mean;       // [C]
    TensorImpl save_var;        // [C]
    uint mode;                  // 0 = train, 1 = eval
    uint batch_size;
    uint channels;
    uint height;
    uint width;
    uint accumulate_grad;
    float momentum;
    float eps;
};

layout(buffer_reference, std430, scalar) buffer ContextBuffer {
    BatchNorm2DContext ctx;
};

layout(push_constant) uniform PushConstants {
    ContextBuffer context;
    uvec3 grid_size;
    uint kernel_type;
} push;

// Shared memory for reduction
shared float shmem[MAX_WG];
shared float shmem2[MAX_WG];
shared float shmem3[MAX_WG];

// Three-value block reduction
void blockReduceSum3(float val1, float val2, float val3, 
                     out float result1, out float result2, out float result3) {
    uint tid = gl_LocalInvocationID.x;
    uint warp_id = tid / WARP_SIZE;
    uint lane_id = tid % WARP_SIZE;
    
    // Warp-level reduction
    val1 = subgroupAdd(val1);
    val2 = subgroupAdd(val2);
    val3 = subgroupAdd(val3);
    
    // First thread in each warp writes to shared memory
    if (lane_id == 0) {
        shmem[warp_id] = val1;
        shmem2[warp_id] = val2;
        shmem3[warp_id] = val3;
    }
    
    barrier();
    
    // First warp reduces the partial sums
    if (warp_id == 0) {
        val1 = (tid < (MAX_WG / WARP_SIZE)) ? shmem[lane_id] : 0.0;
        val2 = (tid < (MAX_WG / WARP_SIZE)) ? shmem2[lane_id] : 0.0;
        val3 = (tid < (MAX_WG / WARP_SIZE)) ? shmem3[lane_id] : 0.0;
        val1 = subgroupAdd(val1);
        val2 = subgroupAdd(val2);
        val3 = subgroupAdd(val3);
    }
    
    result1 = val1;
    result2 = val2;
    result3 = val3;
}

void backward_kernel() {
    BatchNorm2DContext ctx = push.context.ctx;
    
    uint channel_idx = gl_WorkGroupID.x; // Channel index [0, C)
    uint tid = gl_LocalInvocationID.x;
    
    uint B = ctx.batch_size;
    uint C = ctx.channels;
    uint H = ctx.height;
    uint W = ctx.width;
    uint spatial_size = H * W;
    uint total_elements = B * spatial_size; // Total elements per channel
    
    if (channel_idx >= C) return;
    
    // Retrieve saved statistics from forward pass
    float mean = ctx.save_mean.data.data[channel_idx];
    float var = ctx.save_var.data.data[channel_idx];
    float inv_std = 1.0 / sqrt(var + ctx.eps);
    float weight = ctx.weight_tensor.data.data[channel_idx];
    
    // First pass: compute gradients for weight and bias
    // Also accumulate terms needed for input gradient
    float grad_weight_sum = 0.0;
    float grad_bias_sum = 0.0;
    float grad_mean_sum = 0.0; // sum of (grad_out * weight)
    
    for (uint elem_idx = tid; elem_idx < total_elements; elem_idx += MAX_WG) {
        uint batch_idx = elem_idx / spatial_size;
        uint spatial_idx = elem_idx % spatial_size;
        uint idx = batch_idx * (C * spatial_size) + channel_idx * spatial_size + spatial_idx;
        
        float input_val = ctx.input_tensor.data.data[idx];
        float grad_out = ctx.out_tensor.grad.grad[idx];
        
        // Normalized input: (x - mean) / sqrt(var + eps)
        float x_normalized = (input_val - mean) * inv_std;
        
        // Gradient w.r.t. weight: sum(grad_out * x_normalized)
        grad_weight_sum += grad_out * x_normalized;
        
        // Gradient w.r.t. bias: sum(grad_out)
        grad_bias_sum += grad_out;
        
        // For input gradient computation
        grad_mean_sum += grad_out * weight;
    }
    
    // Reduce across block
    float grad_weight, grad_bias, grad_mean;
    blockReduceSum3(grad_weight_sum, grad_bias_sum, grad_mean_sum, 
                    grad_weight, grad_bias, grad_mean);
    
    barrier();
    
    // Thread 0 writes gradients for weight and bias
    if (tid == 0) {
        if (ctx.accumulate_grad == 1) {
            atomicAdd(ctx.weight_tensor.grad.grad[channel_idx], grad_weight * (1.0f/float(B)));
            atomicAdd(ctx.bias_tensor.grad.grad[channel_idx], grad_bias * (1.0f/float(B)));
        } else {
            ctx.weight_tensor.grad.grad[channel_idx] = grad_weight * (1.0f/float(B));
            ctx.bias_tensor.grad.grad[channel_idx] = grad_bias * (1.0f/float(B));
        }
        
        // Store intermediate values in shared memory
        shmem[0] = grad_mean; // sum of (grad_out * weight)
    }
    
    barrier();
    
    // Second pass: compute gradient for input
    // Using the formula:
    // grad_input = (grad_out * weight - mean(grad_out * weight) 
    //              - x_normalized * mean(grad_out * weight * x_normalized)) * inv_std
    
    float grad_mean_val = shmem[0];
    float grad_mean_avg = grad_mean_val / float(total_elements);
    
    // Compute sum of (grad_out * weight * x_normalized) for the variance term
    float grad_var_sum = 0.0;
    
    for (uint elem_idx = tid; elem_idx < total_elements; elem_idx += MAX_WG) {
        uint batch_idx = elem_idx / spatial_size;
        uint spatial_idx = elem_idx % spatial_size;
        uint idx = batch_idx * (C * spatial_size) + channel_idx * spatial_size + spatial_idx;
        
        float input_val = ctx.input_tensor.data.data[idx];
        float grad_out = ctx.out_tensor.grad.grad[idx];
        float x_normalized = (input_val - mean) * inv_std;
        
        grad_var_sum += grad_out * weight * x_normalized;
    }
    
    // Reduce grad_var_sum across block
    grad_var_sum = subgroupAdd(grad_var_sum);
    
    uint warp_id = tid / WARP_SIZE;
    uint lane_id = tid % WARP_SIZE;
    
    if (lane_id == 0) {
        shmem[warp_id] = grad_var_sum;
    }
    
    barrier();
    
    if (warp_id == 0) {
        grad_var_sum = (tid < (MAX_WG / WARP_SIZE)) ? shmem[lane_id] : 0.0;
        grad_var_sum = subgroupAdd(grad_var_sum);
        if (tid == 0) {
            shmem[1] = grad_var_sum;
        }
    }
    
    barrier();
    
    float grad_var_avg = shmem[1] / float(total_elements);
    
    // Third pass: compute and write input gradients
    for (uint elem_idx = tid; elem_idx < total_elements; elem_idx += MAX_WG) {
        uint batch_idx = elem_idx / spatial_size;
        uint spatial_idx = elem_idx % spatial_size;
        uint idx = batch_idx * (C * spatial_size) + channel_idx * spatial_size + spatial_idx;
        
        float input_val = ctx.input_tensor.data.data[idx];
        float grad_out = ctx.out_tensor.grad.grad[idx];
        float x_normalized = (input_val - mean) * inv_std;
        
        // Gradient formula:
        // grad_input = (1/sqrt(var+eps)) * (grad_out * weight 
        //              - mean(grad_out * weight) 
        //              - x_normalized * mean(grad_out * weight * x_normalized))
        float grad_input = (grad_out * weight - grad_mean_avg - x_normalized * grad_var_avg) * inv_std;
        
        if (ctx.accumulate_grad == 1) {
            atomicAdd(ctx.input_tensor.grad.grad[idx], grad_input);
            ctx.out_tensor.grad.grad[idx] = 0.0f;
        } else {
            ctx.input_tensor.grad.grad[idx] = grad_input;
            ctx.out_tensor.grad.grad[idx] = 0.0f;
        }
    }
}

void main() {
    backward_kernel();
}