#version 450
#extension GL_ARB_separate_shader_objects : enable
#extension GL_EXT_buffer_reference : enable
#extension GL_EXT_buffer_reference_uvec2 : enable
#extension GL_EXT_scalar_block_layout : enable
#extension GL_EXT_shader_atomic_float : enable
#extension GL_KHR_shader_subgroup_basic : enable

// Warp-tiling parameters (must match forward pass)
#define BM 128      // Block tile size M
#define BN 128      // Block tile size N
#define BK 8        // Block tile size K
#define WM 64       // Warp tile size M
#define WN 32       // Warp tile size N
#define WNITER 2    // Number of subwarp tiling steps in N dimension
#define TM 8        // Thread tile size M
#define TN 4        // Thread tile size N
#define NUM_THREADS 256
#define WARPSIZE 32

// Kernel type constants
#define KERNEL_INPUT_GRAD 0
#define KERNEL_WEIGHT_GRAD 1

layout(local_size_x = NUM_THREADS, local_size_y = 1, local_size_z = 1) in;

layout(buffer_reference, std430, scalar) buffer tensor_data {
    float data[];
};

layout(buffer_reference, std430, scalar) buffer tensor_grad {
    float grad[];
};

layout(buffer_reference, std430, scalar) buffer tensor_strides {
    int strides[];
};

layout(buffer_reference, std430, scalar) buffer tensor_shape {
    int dims[];
};

struct TensorImpl {
    tensor_data    data;
    tensor_grad    grad;
    tensor_strides strides;
    tensor_shape   shape;
    uint numel, ndim, requires_grad, is_leaf;
};

struct TransposedConv2DContext {
    TensorImpl input_tensor;   // [B, C_in, H_in, W_in]
    TensorImpl weight_tensor;  // [C_in, C_out, KH, KW]
    TensorImpl bias_tensor;    // [C_out]
    TensorImpl out_tensor;     // [B, C_out, H_out, W_out]
    
    uint batch_size;
    uint in_channels;
    uint out_channels;
    uint in_height;
    uint in_width;
    uint out_height;
    uint out_width;
    uint kernel_h;
    uint kernel_w;
    uint stride_h;
    uint stride_w;
    uint padding_h;
    uint padding_w;
    uint dilation_h;
    uint dilation_w;
    uint output_padding_h;
    uint output_padding_w;
    
    uint use_bias;
    uint accumulate_grad;
    uint kernel_type;
    
    // GEMM dimensions for TransposedConv2d
    // M = in_height * in_width (input spatial)
    // N = in_channels
    // K = out_channels * kernel_h * kernel_w
    uint m, n, k;
};

layout(buffer_reference, std430, buffer_reference_align = 16) buffer ContextBuffer { 
    TransposedConv2DContext ctx; 
};

layout(push_constant) uniform PushConstants {
    ContextBuffer context;
    uvec3 grid_size;
    uint  kernel_type;
} push;

// Shared memory for block tiles
shared float As[BM * BK];
shared float Bs[BK * BN];

// Calculated constants
const uint WMITER = (WM * WN) / (WARPSIZE * TM * TN * WNITER);  // 2
const uint WSUBM = WM / WMITER;  // 32
const uint WSUBN = WN / WNITER;  // 16

// Helper function to get tensor offset
uint getTensorOffset(const TensorImpl t, uint idx0, uint idx1, uint idx2, uint idx3)
{
    if (t.ndim == 4u) {
        return idx0 * t.strides.strides[0] + 
               idx1 * t.strides.strides[1] + 
               idx2 * t.strides.strides[2] + 
               idx3 * t.strides.strides[3];
    } else if (t.ndim == 3u) {
        return idx0 * t.strides.strides[0] + 
               idx1 * t.strides.strides[1] + 
               idx2 * t.strides.strides[2];
    } else if (t.ndim == 2u) {
        return idx0 * t.strides.strides[0] + 
               idx1 * t.strides.strides[1];
    } else {
        return idx0 * t.strides.strides[0];
    }
}

// ========================================================================
// Helper: Process from shared memory (warp-tiled computation)
// ========================================================================
void processFromSmem(inout float threadResults[WMITER * TM * WNITER * TN],
                     uint warpRow, uint warpCol,
                     uint threadRowInWarp, uint threadColInWarp)
{
    float regM[WMITER * TM];
    float regN[WNITER * TN];
    
    for (uint dotIdx = 0; dotIdx < BK; ++dotIdx) {
        for (uint wSubRowIdx = 0; wSubRowIdx < WMITER; ++wSubRowIdx) {
            for (uint i = 0; i < TM; ++i) {
                regM[wSubRowIdx * TM + i] = 
                    As[(dotIdx * BM) + warpRow * WM + wSubRowIdx * WSUBM + 
                       threadRowInWarp * TM + i];
            }
        }
        
        for (uint wSubColIdx = 0; wSubColIdx < WNITER; ++wSubColIdx) {
            for (uint i = 0; i < TN; ++i) {
                regN[wSubColIdx * TN + i] = 
                    Bs[(dotIdx * BN) + warpCol * WN + wSubColIdx * WSUBN + 
                       threadColInWarp * TN + i];
            }
        }
        
        for (uint wSubRowIdx = 0; wSubRowIdx < WMITER; ++wSubRowIdx) {
            for (uint wSubColIdx = 0; wSubColIdx < WNITER; ++wSubColIdx) {
                for (uint resIdxM = 0; resIdxM < TM; ++resIdxM) {
                    for (uint resIdxN = 0; resIdxN < TN; ++resIdxN) {
                        threadResults[(wSubRowIdx * TM + resIdxM) * (WNITER * TN) +
                                     (wSubColIdx * TN) + resIdxN] +=
                            regM[wSubRowIdx * TM + resIdxM] * 
                            regN[wSubColIdx * TN + resIdxN];
                    }
                }
            }
        }
    }
}

// ========================================================================
// Kernel 1: Compute dInput = dOutput col2im convolution with Weight
// For TransposedConv2d backward:
// dInput: [B, C_in, H_in, W_in]
// dOutput: [B, C_out, H_out, W_out]
// Weight: [C_in, C_out, KH, KW]
// 
// This is essentially a regular convolution of dOutput with Weight (transposed channels)
// dInput[b, ic, ih, iw] = Σ dOutput[b, oc, oh, ow] * Weight[ic, oc, kh, kw]
// where (oh, ow) = (ih * stride - padding + kh * dilation, iw * stride - padding + kw * dilation)
// ========================================================================
void computeInputGradient(uint B, uint M, uint N, uint K) {
    TransposedConv2DContext c = push.context.ctx;
    
    const uint cRow = gl_WorkGroupID.y;
    const uint cCol = gl_WorkGroupID.x;
    const uint batch = gl_WorkGroupID.z;
    
    const uint warpIdx = gl_LocalInvocationID.x / WARPSIZE;
    const uint warpCol = warpIdx % (BN / WN);
    const uint warpRow = warpIdx / (BN / WN);
    
    const uint threadIdxInWarp = gl_LocalInvocationID.x % WARPSIZE;
    const uint threadColInWarp = threadIdxInWarp % (WSUBN / TN);
    const uint threadRowInWarp = threadIdxInWarp / (WSUBN / TN);
    
    const uint innerRowA = gl_LocalInvocationID.x / (BK / 4);
    const uint innerColA = gl_LocalInvocationID.x % (BK / 4);
    const uint innerRowB = gl_LocalInvocationID.x / (BN / 4);
    const uint innerColB = gl_LocalInvocationID.x % (BN / 4);
    
    const uint rowStrideA = (NUM_THREADS * 4) / BK;
    const uint rowStrideB = NUM_THREADS / (BN / 4);
    
    float threadResults[WMITER * TM * WNITER * TN];
    for (uint i = 0; i < WMITER * TM * WNITER * TN; ++i) {
        threadResults[i] = 0.0;
    }
    
    // Tile across K dimension (out_channels * kernel_h * kernel_w)
    for (uint kIdx = 0; kIdx < K; kIdx += BK) {
        // Load dOutput via col2im-like indexing: [M, K] transposed
        // M = in_h * in_w, K = out_c * kh * kw
        for (uint offset = 0; offset < BM; offset += rowStrideA) {
            uint globalRow = cRow * BM + innerRowA + offset;  // M dimension (input spatial)
            uint globalCol = kIdx + innerColA * 4;             // K dimension
            
            vec4 tmp = vec4(0.0);
            if (globalRow < M && globalCol < K && batch < B) {
                // Decompose globalRow into input spatial coordinates
                uint in_w = globalRow % c.in_width;
                uint in_h = globalRow / c.in_width;
                
                // Load dOutput values via col2im mapping
                for (uint k = 0; k < 4; ++k) {
                    uint col_idx = globalCol + k;
                    if (col_idx < K) {
                        // Decompose col_idx into (out_c, kh, kw)
                        uint kw = col_idx % c.kernel_w;
                        uint temp_k = col_idx / c.kernel_w;
                        uint kh = temp_k % c.kernel_h;
                        uint out_c = temp_k / c.kernel_h;
                        
                        // Compute output spatial coordinates
                        int out_h = int(in_h * c.stride_h) - int(c.padding_h) + int(kh * c.dilation_h);
                        int out_w = int(in_w * c.stride_w) - int(c.padding_w) + int(kw * c.dilation_w);
                        
                        // Check bounds and load
                        if (out_h >= 0 && out_h < int(c.out_height) && 
                            out_w >= 0 && out_w < int(c.out_width)) {
                            uint idx = getTensorOffset(c.out_tensor, batch, out_c, uint(out_h), uint(out_w));
                            float val = c.out_tensor.grad.grad[idx];
                            if (k == 0) tmp.x = val;
                            else if (k == 1) tmp.y = val;
                            else if (k == 2) tmp.z = val;
                            else if (k == 3) tmp.w = val;
                        }
                    }
                }
            }
            
            // Store transposed: As[k][m]
            As[(innerColA * 4 + 0) * BM + innerRowA + offset] = tmp.x;
            As[(innerColA * 4 + 1) * BM + innerRowA + offset] = tmp.y;
            As[(innerColA * 4 + 2) * BM + innerRowA + offset] = tmp.z;
            As[(innerColA * 4 + 3) * BM + innerRowA + offset] = tmp.w;
        }
        
        // Load Weight: [K, N] where K = out_c * kh * kw, N = in_channels
        // Weight is stored as [in_c, out_c, kh, kw], so we need [out_c, kh, kw] -> in_c mapping
        // This is non-contiguous, need scalar loads
        const uint totalElements = BK * BN;
        const uint elemsPerThread = (totalElements + NUM_THREADS - 1) / NUM_THREADS;
        
        for (uint e = 0; e < elemsPerThread; ++e) {
            uint linearIdx = gl_LocalInvocationID.x * elemsPerThread + e;
            if (linearIdx >= totalElements) break;
            
            uint smem_k = linearIdx / BN;
            uint smem_n = linearIdx % BN;
            
            uint globalK = kIdx + smem_k;      // K dimension
            uint globalN = cCol * BN + smem_n; // N dimension (in_channels)
            
            float value = 0.0;
            
            if (globalK < K && globalN < N) {
                // Decompose K into (out_c, kh, kw)
                uint kw = globalK % c.kernel_w;
                uint temp_k = globalK / c.kernel_w;
                uint kh = temp_k % c.kernel_h;
                uint out_c = temp_k / c.kernel_h;
                
                uint in_c = globalN;
                
                // Weight[in_c, out_c, kh, kw]
                uint weightIdx = getTensorOffset(c.weight_tensor, in_c, out_c, kh, kw);
                value = c.weight_tensor.data.data[weightIdx];
            }
            
            Bs[smem_k * BN + smem_n] = value;
        }
        
        barrier();
        
        processFromSmem(threadResults, warpRow, warpCol, threadRowInWarp, threadColInWarp);
        
        barrier();
    }
    
    // Write results to dInput [M, N] -> [B, C_in, H_in, W_in]
    for (uint wSubRowIdx = 0; wSubRowIdx < WMITER; ++wSubRowIdx) {
        for (uint wSubColIdx = 0; wSubColIdx < WNITER; ++wSubColIdx) {
            for (uint resIdxM = 0; resIdxM < TM; ++resIdxM) {
                for (uint resIdxN = 0; resIdxN < TN; ++resIdxN) {
                    uint globalRow = cRow * BM + warpRow * WM + wSubRowIdx * WSUBM + 
                                    threadRowInWarp * TM + resIdxM;
                    uint globalCol = cCol * BN + warpCol * WN + wSubColIdx * WSUBN + 
                                    threadColInWarp * TN + resIdxN;
                    
                    if (globalRow < M && globalCol < N && batch < B) {
                        uint i = (wSubRowIdx * TM + resIdxM) * (WNITER * TN) +
                                wSubColIdx * TN + resIdxN;
                        
                        // Decompose to input position
                        uint in_w = globalRow % c.in_width;
                        uint in_h = globalRow / c.in_width;
                        uint in_c = globalCol;
                        
                        uint idx = getTensorOffset(c.input_tensor, batch, in_c, in_h, in_w);
                        
                        if (c.accumulate_grad == 0u && batch == 0u) {
                            c.input_tensor.grad.grad[idx] = threadResults[i];
                        } else {
                            atomicAdd(c.input_tensor.grad.grad[idx], threadResults[i]);
                        }
                    }
                }
            }
        }
    }
}

// ========================================================================
// Kernel 2: Compute dWeight
// For TransposedConv2d backward:
// dWeight: [C_in, C_out, KH, KW]
// Input: [B, C_in, H_in, W_in]
// dOutput: [B, C_out, H_out, W_out]
//
// dWeight[ic, oc, kh, kw] = Σ Input[b, ic, ih, iw] * dOutput[b, oc, oh, ow]
// where (oh, ow) = (ih * stride - padding + kh * dilation, iw * stride - padding + kw * dilation)
// ========================================================================
void computeWeightGradient(uint B, uint M, uint N, uint K) {
    TransposedConv2DContext c = push.context.ctx;
    
    const uint cRow = gl_WorkGroupID.y;
    const uint cCol = gl_WorkGroupID.x;
    const uint batch = gl_WorkGroupID.z;
    
    const uint warpIdx = gl_LocalInvocationID.x / WARPSIZE;
    const uint warpCol = warpIdx % (BN / WN);
    const uint warpRow = warpIdx / (BN / WN);
    
    const uint threadIdxInWarp = gl_LocalInvocationID.x % WARPSIZE;
    const uint threadColInWarp = threadIdxInWarp % (WSUBN / TN);
    const uint threadRowInWarp = threadIdxInWarp / (WSUBN / TN);
    
    const uint innerRowA = gl_LocalInvocationID.x / (BK / 4);
    const uint innerColA = gl_LocalInvocationID.x % (BK / 4);
    const uint innerRowB = gl_LocalInvocationID.x / (BN / 4);
    const uint innerColB = gl_LocalInvocationID.x % (BN / 4);
    
    const uint rowStrideA = (NUM_THREADS * 4) / BK;
    const uint rowStrideB = NUM_THREADS / (BN / 4);
    
    float threadResults[WMITER * TM * WNITER * TN];
    for (uint i = 0; i < WMITER * TM * WNITER * TN; ++i) {
        threadResults[i] = 0.0;
    }
    
    // GEMM: dWeight[N, K] = Input^T[N, M] @ dOutput_col[M, K]
    // Where N = in_channels, M = in_h * in_w, K = out_c * kh * kw
    // Tile across M dimension (reduction over input spatial positions)
    for (uint mIdx = 0; mIdx < M; mIdx += BK) {
        // Load Input^T: [N, M] where N = in_channels, M = in_h * in_w
        for (uint offset = 0; offset < BM; offset += rowStrideA) {
            uint in_c = cRow * BM + innerRowA + offset;  // N dimension (in_channels)
            uint spatial_idx = mIdx + innerColA * 4;     // M dimension (spatial)
            
            vec4 tmp = vec4(0.0);
            if (spatial_idx < M && in_c < N && batch < B) {
                // Decompose spatial index
                uint in_w = spatial_idx % c.in_width;
                uint in_h = spatial_idx / c.in_width;
                
                uint idx0 = getTensorOffset(c.input_tensor, batch, in_c, in_h, in_w);
                tmp.x = c.input_tensor.data.data[idx0];
                
                if (spatial_idx + 1 < M) {
                    uint in_w1 = (spatial_idx + 1) % c.in_width;
                    uint in_h1 = (spatial_idx + 1) / c.in_width;
                    uint idx1 = getTensorOffset(c.input_tensor, batch, in_c, in_h1, in_w1);
                    tmp.y = c.input_tensor.data.data[idx1];
                }
                if (spatial_idx + 2 < M) {
                    uint in_w2 = (spatial_idx + 2) % c.in_width;
                    uint in_h2 = (spatial_idx + 2) / c.in_width;
                    uint idx2 = getTensorOffset(c.input_tensor, batch, in_c, in_h2, in_w2);
                    tmp.z = c.input_tensor.data.data[idx2];
                }
                if (spatial_idx + 3 < M) {
                    uint in_w3 = (spatial_idx + 3) % c.in_width;
                    uint in_h3 = (spatial_idx + 3) / c.in_width;
                    uint idx3 = getTensorOffset(c.input_tensor, batch, in_c, in_h3, in_w3);
                    tmp.w = c.input_tensor.data.data[idx3];
                }
            }
            
            As[(innerColA * 4 + 0) * BM + innerRowA + offset] = tmp.x;
            As[(innerColA * 4 + 1) * BM + innerRowA + offset] = tmp.y;
            As[(innerColA * 4 + 2) * BM + innerRowA + offset] = tmp.z;
            As[(innerColA * 4 + 3) * BM + innerRowA + offset] = tmp.w;
        }
        
        // Load dOutput via col2im: [M, K] where M = in_h * in_w, K = out_c * kh * kw
        for (uint offset = 0; offset < BK; offset += rowStrideB) {
            uint spatial_idx = mIdx + innerRowB + offset;  // M dimension
            uint k_idx = cCol * BN + innerColB * 4;        // K dimension
            
            vec4 tmp = vec4(0.0);
            if (spatial_idx < M && k_idx < K) {
                // Decompose spatial index
                uint in_w = spatial_idx % c.in_width;
                uint in_h = spatial_idx / c.in_width;
                
                // For each K position, load dOutput via mapping
                for (uint k = 0; k < 4; ++k) {
                    uint col_idx = k_idx + k;
                    if (col_idx < K) {
                        // Decompose K into (out_c, kh, kw)
                        uint kw = col_idx % c.kernel_w;
                        uint temp_k = col_idx / c.kernel_w;
                        uint kh = temp_k % c.kernel_h;
                        uint out_c = temp_k / c.kernel_h;
                        
                        // Compute output spatial coordinates
                        int out_h = int(in_h * c.stride_h) - int(c.padding_h) + int(kh * c.dilation_h);
                        int out_w = int(in_w * c.stride_w) - int(c.padding_w) + int(kw * c.dilation_w);
                        
                        if (out_h >= 0 && out_h < int(c.out_height) && 
                            out_w >= 0 && out_w < int(c.out_width)) {
                            uint idx = getTensorOffset(c.out_tensor, batch, out_c, uint(out_h), uint(out_w));
                            float val = c.out_tensor.grad.grad[idx];
                            if (k == 0) tmp.x = val;
                            else if (k == 1) tmp.y = val;
                            else if (k == 2) tmp.z = val;
                            else if (k == 3) tmp.w = val;
                        }
                    }
                }
            }
            
            uint smemIdx = (innerRowB + offset) * BN + innerColB * 4;
            Bs[smemIdx + 0] = tmp.x;
            Bs[smemIdx + 1] = tmp.y;
            Bs[smemIdx + 2] = tmp.z;
            Bs[smemIdx + 3] = tmp.w;
        }
        
        barrier();
        
        processFromSmem(threadResults, warpRow, warpCol, threadRowInWarp, threadColInWarp);
        
        barrier();
    }
    
    // Write results to dWeight [N, K] -> [C_in, C_out, KH, KW]
    for (uint wSubRowIdx = 0; wSubRowIdx < WMITER; ++wSubRowIdx) {
        for (uint wSubColIdx = 0; wSubColIdx < WNITER; ++wSubColIdx) {
            for (uint resIdxM = 0; resIdxM < TM; ++resIdxM) {
                for (uint resIdxN = 0; resIdxN < TN; ++resIdxN) {
                    uint globalRow = cRow * BM + warpRow * WM + wSubRowIdx * WSUBM + 
                                    threadRowInWarp * TM + resIdxM;
                    uint globalCol = cCol * BN + warpCol * WN + wSubColIdx * WSUBN + 
                                    threadColInWarp * TN + resIdxN;
                    
                    if (globalRow < N && globalCol < K && batch < B) {
                        uint i = (wSubRowIdx * TM + resIdxM) * (WNITER * TN) +
                                wSubColIdx * TN + resIdxN;
                        
                        // Decompose: globalRow = in_c, globalCol = (out_c, kh, kw)
                        uint in_c = globalRow;
                        uint kw = globalCol % c.kernel_w;
                        uint temp = globalCol / c.kernel_w;
                        uint kh = temp % c.kernel_h;
                        uint out_c = temp / c.kernel_h;
                        
                        // Weight[in_c, out_c, kh, kw]
                        uint idx = getTensorOffset(c.weight_tensor, in_c, out_c, kh, kw);
                        
                        if (c.accumulate_grad == 0u && batch == 0u) {
                            c.weight_tensor.grad.grad[idx] = threadResults[i];
                        } else {
                            atomicAdd(c.weight_tensor.grad.grad[idx], threadResults[i]);
                        }
                    }
                }
            }
        }
    }
}

// ========================================================================
// Bias gradient computation (separate simple reduction kernel)
// Sum dOutput over batch and spatial dimensions for each output channel
// ========================================================================
void computeBiasGradient() {
    TransposedConv2DContext c = push.context.ctx;
    
    uint tid = gl_GlobalInvocationID.x;
    uint out_c = tid;
    
    if (out_c >= c.out_channels) return;
    
    float sum = 0.0;
    
    // Reduce over batch and spatial dimensions
    for (uint b = 0; b < c.batch_size; ++b) {
        for (uint h = 0; h < c.out_height; ++h) {
            for (uint w = 0; w < c.out_width; ++w) {
                uint idx = getTensorOffset(c.out_tensor, b, out_c, h, w);
                sum += c.out_tensor.grad.grad[idx];
            }
        }
    }
    
    uint biasIdx = out_c;
    
    if (c.accumulate_grad == 0u) {
        c.bias_tensor.grad.grad[biasIdx] = sum;
    } else {
        atomicAdd(c.bias_tensor.grad.grad[biasIdx], sum);
    }
}

void main() {
    TransposedConv2DContext c = push.context.ctx;
    uint B = max(c.batch_size, 1u);
    uint M = c.m;  // in_height * in_width
    uint N = c.n;  // in_channels
    uint K = c.k;  // out_channels * kernel_h * kernel_w

    if (c.kernel_type == KERNEL_INPUT_GRAD) {
        computeInputGradient(B, M, N, K);
        
        // Compute bias gradient if needed (use first workgroup only)
        if (c.use_bias != 0u && gl_WorkGroupID.x == 0u && gl_WorkGroupID.y == 0u && gl_WorkGroupID.z == 0u) {
            computeBiasGradient();
        }
    } else if (c.kernel_type == KERNEL_WEIGHT_GRAD) {
        computeWeightGradient(B, M, N, K);
    }
}