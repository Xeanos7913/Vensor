#version 450
#extension GL_ARB_separate_shader_objects : enable
#extension GL_EXT_buffer_reference : enable
#extension GL_EXT_buffer_reference_uvec2 : enable
#extension GL_EXT_scalar_block_layout : enable

// Forward declare workgroup size
layout(local_size_x_id = 0, local_size_y_id = 1, local_size_z_id = 2) in;

layout(buffer_reference, std430, buffer_reference_align = 16) buffer tensor_data {
    float data[];
};

layout(buffer_reference, std430, buffer_reference_align = 16) buffer tensor_grad {
    float grad[];
};

layout(buffer_reference, std430, buffer_reference_align = 16) buffer tensor_strides {
    int strides[];
};

layout(buffer_reference, std430, buffer_reference_align = 16) buffer tensor_shape {
    int dims[];
};

struct TensorImpl {
    tensor_data    data;
    tensor_grad    grad;
    tensor_strides strides;
    tensor_shape   shape;
    uint numel, ndim, requires_grad, is_leaf;
};

struct TensorFillContext {
    TensorImpl tensor;    // Tensor to fill with random values
    uint type;            // 0 = Gaussian, 1 = uniform
    uint m, n;            // dimensions of the tensor (for 2D) or first two dims (for 3D)
    float start;            // min value for random fill
    float step;            // max value for random fill
};

layout(buffer_reference, std430, buffer_reference_align = 16) buffer ContextBuffer { 
    TensorFillContext ctx; 
};

layout(push_constant) uniform PushConstants {
    ContextBuffer context;
    uvec3 grid_size;
    uint  kernel_type; // Redundant here. This shader is not part of training optimisation. It's used to initalize values.
} push;

uint off2or3(const TensorImpl t, uint b, uint i, uint j)
{
    if (t.ndim == 2u) {
        return i * t.strides.strides[0] + j * t.strides.strides[1];
    } else {
        return b * t.strides.strides[0] + i * t.strides.strides[1] + j * t.strides.strides[2];
    }
}

void main(){
    // Calculate the indices for the tensor
    uint bx = gl_WorkGroupID.x;  
    uint by = gl_WorkGroupID.y;  
    uint bz = gl_WorkGroupID.z;  
    uint lx = gl_LocalInvocationID.x;  
    uint ly = gl_LocalInvocationID.y;  
    uint b = bz;  
    uint row = by * gl_WorkGroupSize.y + ly; // in [0..M)  
    uint col = bx * gl_WorkGroupSize.x + lx; // in [0..N)

    uint index = off2or3(push.context.ctx.tensor, b, row, col);   

    uint M = push.context.ctx.m;
    uint N = push.context.ctx.n;

    // Early exit for threads that are completely out of bounds  
    if (row >= M || col >= N) {  
        return;  
    }

    if (row < push.context.ctx.m && col < push.context.ctx.n) {
        if (push.context.ctx.type == 0u) {
            // Fill with a linear range of values for testing
            float value = push.context.ctx.start + float(index) * push.context.ctx.step;
            push.context.ctx.tensor.data.data[index] = value;
        } else if (push.context.ctx.type == 1u) {
            // Fill with a linear range of values for testing
            float value = push.context.ctx.start + float(index) * push.context.ctx.step;
            push.context.ctx.tensor.data.data[index] = value;
        }
    }
}