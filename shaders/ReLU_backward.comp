#version 450
#extension GL_ARB_separate_shader_objects : enable
#extension GL_EXT_buffer_reference : enable
#extension GL_EXT_buffer_reference_uvec2 : enable
#extension GL_EXT_scalar_block_layout : enable

// Fixed workgroup size: local_size_x = 256 as requested
layout(local_size_x = 256, local_size_y = 1, local_size_z = 1) in;

layout(buffer_reference, std430, scalar) buffer tensor_data {
    float data[];
};

layout(buffer_reference, std430, scalar) buffer tensor_grad {
    float grad[];
};

layout(buffer_reference, std430, scalar) buffer tensor_strides {
    int strides[];
};

layout(buffer_reference, std430, scalar) buffer tensor_shape {
    int dims[];
};

struct TensorImpl {
    tensor_data    data;
    tensor_grad    grad;
    tensor_strides strides;
    tensor_shape   shape;
    uint numel;
    uint ndim;
    uint requires_grad;
    uint is_leaf;
};

struct ReLUcontext {
    TensorImpl input_tensor;
    TensorImpl output_tensor;
    uint M, N;
    uint batch_size;
    uint mode;
    uint accumulate_grad;
};

layout(buffer_reference, std430, buffer_reference_align = 16) buffer ContextBuffer {
    ReLUcontext ctx;
};

layout(push_constant) uniform PushConstants {
    ContextBuffer context;   // host must match std430 layout
    uvec3 grid_size;         // number of workgroups dispatched (gl_NumWorkGroups)
    uint  kernel_type;       // 0 = forward, 1 = backward (compute dInput)
} push;

// Convert a logical flat index (0..numel-1) into the memory offset using the tensor's shape & strides.
// Assumes dims & strides arrays are indexed 0..ndim-1 where dims[0] is the most-significant dimension.
uint linear_index_to_offset(const TensorImpl t, uint linear_idx)
{
    uint idx = linear_idx;
    uint offset = 0u;
    // iterate from last dimension down to 0
    for (uint di = t.ndim; di > 0u; ) {
        di--;
        uint dim = uint(t.shape.dims[di]);
        uint coord = (dim > 0u) ? (idx % dim) : 0u;
        idx = (dim > 0u) ? (idx / dim) : idx;
        offset += coord * uint(t.strides.strides[di]);
    }
    return offset;
}

void relu_backward()
{
    ReLUcontext c = push.context.ctx;
    uint gid = gl_GlobalInvocationID.x;
    uint total = c.input_tensor.numel;

    if (gid >= total) {
        return;
    }

    uint inOff  = linear_index_to_offset(c.input_tensor, gid);
    uint outOff = linear_index_to_offset(c.output_tensor, gid);

    float inputValue = c.input_tensor.data.data[inOff];

    float outputGrad = 0.0;
    if (c.output_tensor.requires_grad == 1u) {
        outputGrad = c.output_tensor.grad.grad[outOff];
        // zero upstream grad after consuming it
        c.output_tensor.grad.grad[outOff] = 0.0;
    }

    float relu_derivative = (inputValue > 0.0) ? 1.0 : 0.0;
    float inputGrad = outputGrad * relu_derivative;

    if (c.input_tensor.requires_grad == 1u) {
        if (c.accumulate_grad != 0u) {
            c.input_tensor.grad.grad[inOff] += inputGrad;
        } else {
            c.input_tensor.grad.grad[inOff] = inputGrad;
        }
    }
}

void main() {
    relu_backward();
}
