#version 450
#extension GL_ARB_separate_shader_objects : enable
#extension GL_EXT_buffer_reference : enable
#extension GL_EXT_buffer_reference_uvec2 : enable
#extension GL_EXT_scalar_block_layout : enable
#extension GL_EXT_shader_atomic_float : enable

#define NUM_THREADS 256
#define BLOCK_M 16
#define BLOCK_DMODEL 64
#define BLOCK_N 16

layout(local_size_x = NUM_THREADS, local_size_y = 1, local_size_z = 1) in;

layout(buffer_reference, std430, scalar) buffer tensor_data {
    float data[];
};

layout(buffer_reference, std430, scalar) buffer tensor_grad {
    float grad[];
};

layout(buffer_reference, std430, scalar) buffer tensor_strides {
    int strides[];
};

layout(buffer_reference, std430, scalar) buffer tensor_shape {
    int dims[];
};

layout(buffer_reference, std430, scalar) buffer DEL {
    float delta[];
};

layout(buffer_reference, std430, scalar) buffer softmaxMaxVals {
    float vals[];
};

struct TensorImpl {
    tensor_data    data;
    tensor_grad    grad;
    tensor_strides strides;
    tensor_shape   shape;
    uint numel, ndim, requires_grad, is_leaf;
};

struct FlashAttentionBackwardContext {
    TensorImpl Q;        // Q.data.data for forward values, Q.grad.grad for gradients (output)
    TensorImpl K;        // K.data.data for forward values, K.grad.grad for gradients (output)
    TensorImpl V;        // V.data.data for forward values, V.grad.grad for gradients (output)
    softmaxMaxVals M;    // Softmax max values
    TensorImpl Out;      // Normalized gradient from preprocess lives in the out tensor
    DEL Delta;    // Delta values from preprocess
    uint N_CTX;
    uint Z;              // batch size
    uint H;              // number of heads
    float sm_scale;      // softmax scale
    uint num_block;      // number of blocks
    int stride_qz, stride_qh, stride_qm, stride_qk;
    int stride_kz, stride_kh, stride_kn, stride_kk;
    int stride_vz, stride_vh, stride_vk, stride_vn;
};

layout(buffer_reference, std430, buffer_reference_align = 16) buffer ContextBuffer { 
    FlashAttentionBackwardContext ctx; 
};

layout(push_constant) uniform PushConstants {
    ContextBuffer context;
    uvec3 grid_size;
    uint  kernel_type;
} push;

// Shared memory tiles
shared float k_tile[BLOCK_M][BLOCK_DMODEL];
shared float v_tile[BLOCK_M][BLOCK_DMODEL];
shared float q_tile[BLOCK_M][BLOCK_DMODEL];
shared float do_tile[BLOCK_M][BLOCK_DMODEL];

// Accumulators for dk and dv
shared float dk_acc[BLOCK_M][BLOCK_DMODEL];
shared float dv_acc[BLOCK_M][BLOCK_DMODEL];

// Temporary storage for intermediate computations
shared float qk_tile[BLOCK_M][BLOCK_N];
shared float p_tile[BLOCK_M][BLOCK_N];
shared float ds_tile[BLOCK_M][BLOCK_N];

shared float m_vals[BLOCK_M];
shared float delta_vals[BLOCK_M];

void load_k_tile(FlashAttentionBackwardContext c, uint off_hz, uint start_n) {
    uint tid = gl_LocalInvocationID.x;
    uint num_loads = (BLOCK_M * BLOCK_DMODEL) / NUM_THREADS;
    
    for (uint i = 0; i < num_loads; i++) {
        uint flat_idx = tid + i * NUM_THREADS;
        uint m_idx = flat_idx / BLOCK_DMODEL;
        uint d_idx = flat_idx % BLOCK_DMODEL;
        
        if (m_idx < BLOCK_M && d_idx < BLOCK_DMODEL) {
            uint offs_n = start_n + m_idx;
            if (offs_n < c.N_CTX) {
                uint k_offset = off_hz * c.stride_kh + offs_n * c.stride_kn + d_idx * c.stride_kk;
                k_tile[m_idx][d_idx] = c.K.data.data[k_offset];
            } else {
                k_tile[m_idx][d_idx] = 0.0;
            }
        }
    }
}

void load_v_tile(FlashAttentionBackwardContext c, uint off_hz, uint start_n) {
    uint tid = gl_LocalInvocationID.x;
    uint num_loads = (BLOCK_M * BLOCK_DMODEL) / NUM_THREADS;
    
    for (uint i = 0; i < num_loads; i++) {
        uint flat_idx = tid + i * NUM_THREADS;
        uint m_idx = flat_idx / BLOCK_DMODEL;
        uint d_idx = flat_idx % BLOCK_DMODEL;
        
        if (m_idx < BLOCK_M && d_idx < BLOCK_DMODEL) {
            uint offs_n = start_n + m_idx;
            if (offs_n < c.N_CTX) {
                uint v_offset = off_hz * c.stride_vh + offs_n * c.stride_vk + d_idx * c.stride_vn;
                v_tile[m_idx][d_idx] = c.V.data.data[v_offset];
            } else {
                v_tile[m_idx][d_idx] = 0.0;
            }
        }
    }
}

void load_q_tile(FlashAttentionBackwardContext c, uint off_hz, uint start_m) {
    uint tid = gl_LocalInvocationID.x;
    uint num_loads = (BLOCK_M * BLOCK_DMODEL) / NUM_THREADS;
    
    for (uint i = 0; i < num_loads; i++) {
        uint flat_idx = tid + i * NUM_THREADS;
        uint m_idx = flat_idx / BLOCK_DMODEL;
        uint d_idx = flat_idx % BLOCK_DMODEL;
        
        if (m_idx < BLOCK_M && d_idx < BLOCK_DMODEL) {
            uint offs_m = start_m + m_idx;
            if (offs_m < c.N_CTX) {
                uint q_offset = off_hz * c.stride_qh + offs_m * c.stride_qm + d_idx * c.stride_qk;
                q_tile[m_idx][d_idx] = c.Q.data.data[q_offset];
            } else {
                q_tile[m_idx][d_idx] = 0.0;
            }
        }
    }
}

void load_do_tile(FlashAttentionBackwardContext c, uint off_hz, uint start_m) {
    uint tid = gl_LocalInvocationID.x;
    uint num_loads = (BLOCK_M * BLOCK_DMODEL) / NUM_THREADS;
    
    for (uint i = 0; i < num_loads; i++) {
        uint flat_idx = tid + i * NUM_THREADS;
        uint m_idx = flat_idx / BLOCK_DMODEL;
        uint d_idx = flat_idx % BLOCK_DMODEL;
        
        if (m_idx < BLOCK_M && d_idx < BLOCK_DMODEL) {
            uint offs_m = start_m + m_idx;
            if (offs_m < c.N_CTX) {
                uint offset = off_hz * c.stride_qh + offs_m * c.stride_qm + d_idx * c.stride_qk;
                do_tile[m_idx][d_idx] = c.Out.grad.grad[offset];
            } else {
                do_tile[m_idx][d_idx] = 0.0;
            }
        }
    }
}

void compute_qk_and_p(FlashAttentionBackwardContext c, uint start_m, uint start_n, uint lo) {
    uint tid = gl_LocalInvocationID.x;
    uint num_elements = BLOCK_M * BLOCK_N;
    uint num_iters = (num_elements + NUM_THREADS - 1) / NUM_THREADS;
    
    for (uint iter = 0; iter < num_iters; iter++) {
        uint flat_idx = tid + iter * NUM_THREADS;
        if (flat_idx < num_elements) {
            uint m_idx = flat_idx / BLOCK_N;
            uint n_idx = flat_idx % BLOCK_N;
            
            // Compute QK^T
            float sum = 0.0;
            for (uint d = 0; d < BLOCK_DMODEL; d++) {
                sum += q_tile[m_idx][d] * k_tile[n_idx][d];
            }
            
            qk_tile[m_idx][n_idx] = sum * c.sm_scale;
            
            // Apply causal mask and compute P = exp(QK - M)
            uint offs_m = start_m + m_idx;
            uint offs_n = start_n + n_idx;
            
            if (offs_m >= offs_n && offs_m < c.N_CTX && offs_n < c.N_CTX) {
                float p_val = exp(qk_tile[m_idx][n_idx] - m_vals[m_idx]);
                p_tile[m_idx][n_idx] = p_val;
            } else {
                p_tile[m_idx][n_idx] = 0.0;
            }
        }
    }
}

void main() {
    FlashAttentionBackwardContext c = push.context.ctx;
    
    uint start_n = gl_WorkGroupID.x * BLOCK_M;  // This workgroup processes a K/V block
    uint off_hz = gl_WorkGroupID.y;
    uint tid = gl_LocalInvocationID.x;
    
    uint off_z = off_hz / c.H;
    uint off_h = off_hz % c.H;
    
    uint lo = start_n;
    
    // Initialize dk and dv accumulators
    uint num_loads = (BLOCK_M * BLOCK_DMODEL) / NUM_THREADS;
    for (uint i = 0; i < num_loads; i++) {
        uint flat_idx = tid + i * NUM_THREADS;
        uint m_idx = flat_idx / BLOCK_DMODEL;
        uint d_idx = flat_idx % BLOCK_DMODEL;
        
        if (m_idx < BLOCK_M && d_idx < BLOCK_DMODEL) {
            dk_acc[m_idx][d_idx] = 0.0;
            dv_acc[m_idx][d_idx] = 0.0;
        }
    }
    
    barrier();
    
    // Load K and V tiles (stay in shared memory throughout)
    load_k_tile(c, off_hz, start_n);
    load_v_tile(c, off_hz, start_n);
    barrier();
    
    // Load M values for the K/V block
    uint rows_per_thread = (BLOCK_M + NUM_THREADS - 1) / NUM_THREADS;
    for (uint i = 0; i < rows_per_thread; i++) {
        uint n_idx = tid + i * NUM_THREADS;
        if (n_idx < BLOCK_M) {
            uint offs_n = start_n + n_idx;
            if (offs_n < c.N_CTX) {
                uint m_offset = off_hz * c.N_CTX + offs_n;
                m_vals[n_idx] = c.M.vals[m_offset];
            } else {
                m_vals[n_idx] = 0.0;
            }
        }
    }
    
    barrier();
    
    // Loop over Q blocks (rows)
    for (uint start_m = lo; start_m < c.num_block * BLOCK_M; start_m += BLOCK_M) {
        if (start_m >= c.N_CTX) break;
        
        // Load Q and DO tiles
        load_q_tile(c, off_hz, start_m);
        load_do_tile(c, off_hz, start_m);
        barrier();
        
        // Load Delta values for this Q block
        for (uint i = 0; i < rows_per_thread; i++) {
            uint m_idx = tid + i * NUM_THREADS;
            if (m_idx < BLOCK_M) {
                uint offs_m = start_m + m_idx;
                if (offs_m < c.N_CTX) {
                    uint delta_offset = off_hz * c.N_CTX + offs_m;
                    delta_vals[m_idx] = c.Delta.delta[delta_offset];
                } else {
                    delta_vals[m_idx] = 0.0;
                }
            }
        }
        barrier();
        
        // Recompute P = softmax(QK^T)
        compute_qk_and_p(c, start_m, start_n, lo);
        barrier();
        
        // Compute dV += P^T @ dO
        for (uint i = 0; i < num_loads; i++) {
            uint flat_idx = tid + i * NUM_THREADS;
            uint n_idx = flat_idx / BLOCK_DMODEL;
            uint d_idx = flat_idx % BLOCK_DMODEL;
            
            if (n_idx < BLOCK_M && d_idx < BLOCK_DMODEL) {
                float sum = 0.0;
                for (uint m = 0; m < BLOCK_M; m++) {
                    sum += p_tile[m][n_idx] * do_tile[m][d_idx];
                }
                dv_acc[n_idx][d_idx] += sum;
            }
        }
        
        // Compute dP = dO @ V^T
        uint dp_elements = BLOCK_M * BLOCK_N;
        uint dp_iters = (dp_elements + NUM_THREADS - 1) / NUM_THREADS;
        
        for (uint iter = 0; iter < dp_iters; iter++) {
            uint flat_idx = tid + iter * NUM_THREADS;
            if (flat_idx < dp_elements) {
                uint m_idx = flat_idx / BLOCK_N;
                uint n_idx = flat_idx % BLOCK_N;
                
                float sum = -delta_vals[m_idx];
                for (uint d = 0; d < BLOCK_DMODEL; d++) {
                    sum += do_tile[m_idx][d] * v_tile[n_idx][d];
                }
                
                // Compute dS = P * dP * sm_scale
                ds_tile[m_idx][n_idx] = p_tile[m_idx][n_idx] * sum * c.sm_scale;
            }
        }
        barrier();
        
        // Compute dK += dS^T @ Q
        for (uint i = 0; i < num_loads; i++) {
            uint flat_idx = tid + i * NUM_THREADS;
            uint n_idx = flat_idx / BLOCK_DMODEL;
            uint d_idx = flat_idx % BLOCK_DMODEL;
            
            if (n_idx < BLOCK_M && d_idx < BLOCK_DMODEL) {
                float sum = 0.0;
                for (uint m = 0; m < BLOCK_M; m++) {
                    sum += ds_tile[m][n_idx] * q_tile[m][d_idx];
                }
                dk_acc[n_idx][d_idx] += sum;
            }
        }
        
        // Compute dQ = dS @ K and accumulate to global memory
        for (uint i = 0; i < num_loads; i++) {
            uint flat_idx = tid + i * NUM_THREADS;
            uint m_idx = flat_idx / BLOCK_DMODEL;
            uint d_idx = flat_idx % BLOCK_DMODEL;
            
            if (m_idx < BLOCK_M && d_idx < BLOCK_DMODEL) {
                uint offs_m = start_m + m_idx;
                if (offs_m < c.N_CTX) {
                    float sum = 0.0;
                    for (uint n = 0; n < BLOCK_M; n++) {
                        sum += ds_tile[m_idx][n] * k_tile[n][d_idx];
                    }
                    
                    uint dq_offset = off_hz * c.stride_qh + offs_m * c.stride_qm + d_idx * c.stride_qk;
                    atomicAdd(c.Q.grad.grad[dq_offset], sum);
                }
            }
        }
        
        barrier();
    }
    
    // Write back dK and dV
    for (uint i = 0; i < num_loads; i++) {
        uint flat_idx = tid + i * NUM_THREADS;
        uint n_idx = flat_idx / BLOCK_DMODEL;
        uint d_idx = flat_idx % BLOCK_DMODEL;
        
        if (n_idx < BLOCK_M && d_idx < BLOCK_DMODEL) {
            uint offs_n = start_n + n_idx;
            if (offs_n < c.N_CTX) {
                uint dk_offset = off_hz * c.stride_kh + offs_n * c.stride_kn + d_idx * c.stride_kk;
                uint dv_offset = off_hz * c.stride_vh + offs_n * c.stride_vk + d_idx * c.stride_vn;
                
                c.K.grad.grad[dk_offset] = dk_acc[n_idx][d_idx];
                c.V.grad.grad[dv_offset] = dv_acc[n_idx][d_idx];
            }
        }
    }
}