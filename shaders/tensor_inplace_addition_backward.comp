#version 450
#extension GL_ARB_separate_shader_objects : enable
#extension GL_EXT_buffer_reference : enable
#extension GL_EXT_buffer_reference_uvec2 : enable
#extension GL_EXT_scalar_block_layout : enable

layout(local_size_x_id = 0, local_size_y_id = 1, local_size_z_id = 2) in;

layout(buffer_reference, std430, scalar) buffer tensor_data {
    float data[];
};

layout(buffer_reference, std430, scalar) buffer tensor_grad {
    float grad[];
};

layout(buffer_reference, std430, scalar) buffer tensor_strides {
    int strides[];
};

layout(buffer_reference, std430, scalar) buffer tensor_shape {
    int dims[];
};

struct TensorImpl {
    tensor_data    data;
    tensor_grad    grad;
    tensor_strides strides;
    tensor_shape   shape;
    uint numel, ndim, requires_grad, is_leaf;
};

struct MataddInplaceContext {
    TensorImpl input_a;      // [B?, M, N] - addend (read-only in forward)
    TensorImpl input_b;      // [B?, M, N] - accumulator (in-place); input_b.grad holds dB
    uint batch_size;
    uint m, n;
    uint accumulate_grad;    // 0: overwrite, 1: += for grads
};

layout(buffer_reference, std430, buffer_reference_align = 16) buffer ContextBuffer { 
    MataddInplaceContext ctx; 
};

layout(push_constant) uniform PushConstants {
    ContextBuffer context;
    uvec3 grid_size;
} push;

// Convert multi-dimensional index to linear index
uint compute_linear_index(uvec3 coords, TensorImpl tensor) {
    uint linear_idx = 0;
    uint batch_idx = coords.z;
    uint row_idx = coords.y;
    uint col_idx = coords.x;
    
    if (tensor.ndim == 2) {
        // 2D tensor: [M, N]
        linear_idx = row_idx * uint(tensor.strides.strides[0]) + col_idx * uint(tensor.strides.strides[1]);
    } else if (tensor.ndim == 3) {
        // 3D tensor: [B, M, N]
        linear_idx = batch_idx * uint(tensor.strides.strides[0]) + 
                     row_idx * uint(tensor.strides.strides[1]) + 
                     col_idx * uint(tensor.strides.strides[2]);
    }
    
    return linear_idx;
}

void main() {
    MataddInplaceContext ctx = push.context.ctx;
    
    uvec3 global_id = gl_GlobalInvocationID;
    uint col = global_id.x;
    uint row = global_id.y;
    uint batch = global_id.z;
    
    // Check bounds
    if (col >= ctx.n || row >= ctx.m || batch >= ctx.batch_size) {
        return;
    }
    
    uvec3 coords = uvec3(col, row, batch);
    
    // Backward pass for in-place addition: B = B + A
    // Forward: B_out = B_in + A
    // Backward: dB_in = dB_out, dA = dB_out
    
    uint idx_b = compute_linear_index(coords, ctx.input_b);
    float db = ctx.input_b.grad.grad[idx_b]; // This is dB_out (gradient of output B)
    
    // Compute gradients for input_a if it requires gradients
    // dA = dB_out (since ∂B_out/∂A = 1)
    if (ctx.input_a.requires_grad != 0) {
        uint idx_a = compute_linear_index(coords, ctx.input_a);
        if (ctx.accumulate_grad != 0) {
            ctx.input_a.grad.grad[idx_a] += db;
        } else {
            ctx.input_a.grad.grad[idx_a] = db;
        }
    }
    
    // For input_b, the gradient flows through unchanged
    // dB_in = dB_out (since ∂B_out/∂B_in = 1)
    // The gradient is already in input_b.grad, so we don't need to write it again
    // unless we need to zero it out after use (but typically for in-place ops,
    // the gradient stays in place for the next backward pass)
    
    // Note: We do NOT zero out input_b.grad here because input_b is the output
    // and its gradient will be used by upstream operations
}