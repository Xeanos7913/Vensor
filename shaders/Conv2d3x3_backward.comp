#version 460
#extension GL_ARB_separate_shader_objects : enable
#extension GL_EXT_buffer_reference : enable
#extension GL_EXT_buffer_reference_uvec2 : enable
#extension GL_EXT_scalar_block_layout : enable
#extension GL_EXT_shader_atomic_float : enable
#extension GL_KHR_shader_subgroup_basic : enable
#extension GL_KHR_shader_subgroup_shuffle : enable
#extension GL_KHR_shader_subgroup_arithmetic : enable
#extension GL_EXT_control_flow_attributes : enable

// Workgroup configuration
#define TILE 16
#define OUT_TILE_X 2
#define OUT_TILE_Y 2
#define THREADS_X (TILE / OUT_TILE_X)
#define THREADS_Y (TILE / OUT_TILE_Y)
#define K 3
#define MAX_PAD 2
#define MAX_DILATION 2
#define MAX_STRIDE 2
#define SHARED_W (TILE * MAX_STRIDE + (K - 1) * MAX_DILATION + MAX_PAD * 2)
#define SHARED_W_PAD (SHARED_W + 1)

layout(local_size_x = THREADS_X, local_size_y = THREADS_Y, local_size_z = 1) in;

layout(buffer_reference, std430, scalar) buffer tensor_data {
    float data[];
};

layout(buffer_reference, std430, scalar) buffer tensor_grad {
    float grad[];
};

layout(buffer_reference, std430, scalar) buffer tensor_strides {
    int strides[];
};

layout(buffer_reference, std430, scalar) buffer tensor_shape {
    int dims[];
};

struct TensorImpl {
    tensor_data data;
    tensor_grad grad;
    tensor_strides strides;
    tensor_shape shape;
    uint numel, ndim, requires_grad, is_leaf;
};

struct Conv2dContext {
    TensorImpl input_tensor;  // [N, C_in, H_in, W_in]
    TensorImpl weight_tensor; // [C_out, C_in, K_h, K_w]
    TensorImpl bias_tensor;   // [C_out]
    TensorImpl out_tensor;    // [N, C_out, H_out, W_out]
    uint stride_h;
    uint stride_w;
    uint pad_h;
    uint pad_w;
    uint dilation_h;
    uint dilation_w;
    uint groups;
    uint kernel_type;   // 0 = gradient wrt input, 1 = weights, 2 = bias 
};

layout(buffer_reference, std430, scalar) buffer ContextBuffer {
    Conv2dContext ctx;
};

layout(push_constant) uniform PushConstants {
    ContextBuffer context;
    uvec3 grid_size;
    uint kernel_type;  // 0: input grad, 1: weight grad, 2: bias grad
} push;

// Shared memory tile
shared float sh[SHARED_W][SHARED_W_PAD];

// Helper function to load 4 floats (vectorized load)
void load4_shared(int sh_x, int sh_y, int global_offset, int valid_count, tensor_grad grad_buffer) {
    vec4 v = vec4(0.0);
    
    if (valid_count >= 4) {
        v.x = grad_buffer.grad[global_offset + 0];
        v.y = grad_buffer.grad[global_offset + 1];
        v.z = grad_buffer.grad[global_offset + 2];
        v.w = grad_buffer.grad[global_offset + 3];
    } else if (valid_count > 0) {
        for (int i = 0; i < valid_count; ++i) {
            v[i] = grad_buffer.grad[global_offset + i];
        }
    }
    
    sh[sh_y][sh_x + 0] = v.x;
    sh[sh_y][sh_x + 1] = v.y;
    sh[sh_y][sh_x + 2] = v.z;
    sh[sh_y][sh_x + 3] = v.w;
}

void load4_shared_data(int sh_x, int sh_y, int global_offset, int valid_count, tensor_data data_buffer) {
    vec4 v = vec4(0.0);
    
    if (valid_count >= 4) {
        v.x = data_buffer.data[global_offset + 0];
        v.y = data_buffer.data[global_offset + 1];
        v.z = data_buffer.data[global_offset + 2];
        v.w = data_buffer.data[global_offset + 3];
    } else if (valid_count > 0) {
        for (int i = 0; i < valid_count; ++i) {
            v[i] = data_buffer.data[global_offset + i];
        }
    }
    
    sh[sh_y][sh_x + 0] = v.x;
    sh[sh_y][sh_x + 1] = v.y;
    sh[sh_y][sh_x + 2] = v.z;
    sh[sh_y][sh_x + 3] = v.w;
}

// KERNEL 0: Compute gradient w.r.t. input
void compute_input_grad() {
    Conv2dContext ctx = push.context.ctx;
    
    int B = int(ctx.input_tensor.shape.dims[0]);
    int C_in = int(ctx.input_tensor.shape.dims[1]);
    int H_in = int(ctx.input_tensor.shape.dims[2]);
    int W_in = int(ctx.input_tensor.shape.dims[3]);
    int C_out = int(ctx.weight_tensor.shape.dims[0]);
    int H_out = int(ctx.out_tensor.shape.dims[2]);
    int W_out = int(ctx.out_tensor.shape.dims[3]);
    
    int stride_h = int(ctx.stride_h);
    int stride_w = int(ctx.stride_w);
    int pad_h = int(ctx.pad_h);
    int pad_w = int(ctx.pad_w);
    int dilation_h = int(ctx.dilation_h);
    int dilation_w = int(ctx.dilation_w);
    
    int block_out_x = int(gl_WorkGroupID.x) * TILE;
    int block_out_y = int(gl_WorkGroupID.y) * TILE;
    int tx = int(gl_LocalInvocationID.x);
    int ty = int(gl_LocalInvocationID.y);
    
    int out_x0 = block_out_x + tx * OUT_TILE_X;
    int out_y0 = block_out_y + ty * OUT_TILE_Y;
    
    int b = int(gl_WorkGroupID.z) / C_in;
    int c = int(gl_WorkGroupID.z) % C_in;
    
    if (b >= B) return;
    
    int grad_out_b_offset = b * C_out * H_out * W_out;
    int grad_input_b_offset = b * C_in * H_in * W_in;
    
    float acc[OUT_TILE_Y][OUT_TILE_X];
    for (int oy = 0; oy < OUT_TILE_Y; ++oy) {
        for (int ox = 0; ox < OUT_TILE_X; ++ox) {
            acc[oy][ox] = 0.0;
        }
    }
    
    // Loop over output channels (filters)
    for (int f = 0; f < C_out; ++f) {
        // Load grad_output tile into shared memory
        // For transposed conv, we need to load a larger region
        int shared_h = TILE + (K - 1) * dilation_h;
        int shared_w = TILE + (K - 1) * dilation_w;
        
        for (int sy = ty; sy < shared_h; sy += THREADS_Y) {
            // Map input position to output position
            int in_y = block_out_y + sy;
            int out_y_start = (in_y + pad_h) / stride_h;
            int out_y_offset = (in_y + pad_h) % stride_h;
            
            for (int sx = tx * 4; sx < shared_w; sx += THREADS_X * 4) {
                int in_x = block_out_x + sx;
                
                int valid = 0;
                // For each input position, determine if it corresponds to a valid output position
                for (int v = 0; v < 4; ++v) {
                    int curr_x = in_x + v;
                    int out_x = (curr_x + pad_w) / stride_w;
                    int out_x_offset = (curr_x + pad_w) % stride_w;
                    
                    if (out_x >= 0 && out_x < W_out && out_y_start >= 0 && out_y_start < H_out &&
                        out_x_offset == 0 && out_y_offset == 0) {
                        int global_offset = grad_out_b_offset + f * H_out * W_out + out_y_start * W_out + out_x;
                        sh[sy][sx + v] = ctx.out_tensor.grad.grad[global_offset];
                    } else {
                        sh[sy][sx + v] = 0.0;
                    }
                }
            }
        }
        
        barrier();
        
        // Convolve with transposed/flipped kernel
        int kbase = (f * C_in + c) * K * K;
        
        for (int oy = 0; oy < OUT_TILE_Y; ++oy) {
            for (int ox = 0; ox < OUT_TILE_X; ++ox) {
                int in_y = out_y0 + oy;
                int in_x = out_x0 + ox;
                
                if (in_y >= H_in || in_x >= W_in) continue;
                
                int sy = in_y - block_out_y;
                int sx = in_x - block_out_x;
                
                float sum = 0.0;
                
                sum = 0.0;
                [[unroll]] for (int ky = 0; ky < K; ++ky) {
                    [[unroll]] for (int kx = 0; kx < K; ++kx) {
                        // Transposed convolution: flip kernel indices (K-1-ky, K-1-kx)
                        sum += sh[sy + ky * dilation_h][sx + kx * dilation_w] * 
                              ctx.weight_tensor.data.data[kbase + (K-1-ky)*K + (K-1-kx)];
                    }
                }
                
                acc[oy][ox] += sum;
            }
        }
        
        barrier();
    }
    
    // Write grad_input
    for (int oy = 0; oy < OUT_TILE_Y; ++oy) {
        int out_y = out_y0 + oy;
        if (out_y < 0 || out_y >= H_in) continue;
        
        for (int ox = 0; ox < OUT_TILE_X; ++ox) {
            int out_x = out_x0 + ox;
            if (out_x < 0 || out_x >= W_in) continue;
            
            int out_idx = grad_input_b_offset + c * (H_in * W_in) + out_y * W_in + out_x;
            atomicAdd(ctx.input_tensor.grad.grad[out_idx], acc[oy][ox]);
        }
    }
}

// KERNEL 1: Compute gradient w.r.t. weights
void compute_weight_grad() {
    Conv2dContext ctx = push.context.ctx;
    
    int B = int(ctx.input_tensor.shape.dims[0]);
    int C_in = int(ctx.input_tensor.shape.dims[1]);
    int H_in = int(ctx.input_tensor.shape.dims[2]);
    int W_in = int(ctx.input_tensor.shape.dims[3]);
    int C_out = int(ctx.weight_tensor.shape.dims[0]);
    int H_out = int(ctx.out_tensor.shape.dims[2]);
    int W_out = int(ctx.out_tensor.shape.dims[3]);
    
    int stride_h = int(ctx.stride_h);
    int stride_w = int(ctx.stride_w);
    int pad_h = int(ctx.pad_h);
    int pad_w = int(ctx.pad_w);
    int dilation_h = int(ctx.dilation_h);
    int dilation_w = int(ctx.dilation_w);
    
    int block_out_x = int(gl_WorkGroupID.x) * TILE;
    int block_out_y = int(gl_WorkGroupID.y) * TILE;
    int tx = int(gl_LocalInvocationID.x);
    int ty = int(gl_LocalInvocationID.y);
    
    int out_x0 = block_out_x + tx * OUT_TILE_X;
    int out_y0 = block_out_y + ty * OUT_TILE_Y;
    
    int b = int(gl_WorkGroupID.z) / C_out;
    int f = int(gl_WorkGroupID.z) % C_out;
    
    if (b >= B) return;
    
    int input_b_offset = b * C_in * H_in * W_in;
    int grad_out_b_offset = b * C_out * H_out * W_out;
    
    // Accumulate gradient for each kernel position
    float grad_k[K][K][OUT_TILE_Y][OUT_TILE_X];
    for (int ky = 0; ky < K; ++ky) {
        for (int kx = 0; kx < K; ++kx) {
            for (int oy = 0; oy < OUT_TILE_Y; ++oy) {
                for (int ox = 0; ox < OUT_TILE_X; ++ox) {
                    grad_k[ky][kx][oy][ox] = 0.0;
                }
            }
        }
    }
    
    // Calculate shared memory size needed
    int kernel_h_eff = (K - 1) * dilation_h + 1;
    int kernel_w_eff = (K - 1) * dilation_w + 1;
    int shared_h = TILE * stride_h + kernel_h_eff - stride_h + pad_h * 2;
    int shared_w = TILE * stride_w + kernel_w_eff - stride_w + pad_w * 2;
    
    // Loop over input channels
    for (int c = 0; c < C_in; ++c) {
        // Load input tile into shared memory
        int in_y_start = block_out_y * stride_h - pad_h;
        int in_x_start = block_out_x * stride_w - pad_w;
        
        for (int sy = ty; sy < shared_h; sy += THREADS_Y) {
            int in_y = in_y_start + sy;
            
            for (int sx = tx * 4; sx < shared_w; sx += THREADS_X * 4) {
                int in_x = in_x_start + sx;
                
                int valid = 0;
                if (in_y >= 0 && in_y < H_in) {
                    if (in_x >= 0 && in_x + 3 < W_in) {
                        valid = 4;
                    } else if (in_x >= 0 && in_x < W_in) {
                        valid = W_in - in_x;
                    }
                }
                
                if (valid > 0) {
                    int global_offset = input_b_offset + c * H_in * W_in + in_y * W_in + in_x;
                    load4_shared_data(sx, sy, global_offset, valid, ctx.input_tensor.data);
                } else {
                    sh[sy][sx + 0] = 0.0;
                    sh[sy][sx + 1] = 0.0;
                    sh[sy][sx + 2] = 0.0;
                    sh[sy][sx + 3] = 0.0;
                }
            }
        }
        
        barrier();
        
        // Compute weight gradient contribution
        for (int oy = 0; oy < OUT_TILE_Y; ++oy) {
            int out_y = out_y0 + oy;
            if (out_y < 0 || out_y >= H_out) continue;
            
            for (int ox = 0; ox < OUT_TILE_X; ++ox) {
                int out_x = out_x0 + ox;
                if (out_x < 0 || out_x >= W_out) continue;
                
                // Position in shared memory
                int sy_base = (out_y - block_out_y) * stride_h + pad_h;
                int sx_base = (out_x - block_out_x) * stride_w + pad_w;
                
                // Load grad_output value
                int grad_idx = grad_out_b_offset + f * (H_out * W_out) + out_y * W_out + out_x;
                float grad_out_val = ctx.out_tensor.grad.grad[grad_idx];
                
                // Accumulate gradient for each kernel position with dilation
                for (int ky = 0; ky < K; ++ky) {
                    for (int kx = 0; kx < K; ++kx) {
                        grad_k[ky][kx][oy][ox] += sh[sy_base + ky * dilation_h][sx_base + kx * dilation_w] * grad_out_val;
                    }
                }
            }
        }
        
        barrier();
        
        // Reduce and write weight gradients
        int kbase = (f * C_in + c) * K * K;
        for (int ky = 0; ky < K; ++ky) {
            for (int kx = 0; kx < K; ++kx) {
                float sum = 0.0;
                for (int oy = 0; oy < OUT_TILE_Y; ++oy) {
                    for (int ox = 0; ox < OUT_TILE_X; ++ox) {
                        sum += grad_k[ky][kx][oy][ox];
                    }
                }
                
                if (sum != 0.0) {
                    int k_idx = kbase + ky * K + kx;
                    atomicAdd(ctx.weight_tensor.grad.grad[k_idx], sum);
                }
            }
        }
    }
}

shared float reduction[THREADS_X * THREADS_Y];

// KERNEL 2: Compute gradient w.r.t. bias
void compute_bias_grad() {
    Conv2dContext ctx = push.context.ctx;
    
    int B = int(ctx.input_tensor.shape.dims[0]);
    int H_out = int(ctx.out_tensor.shape.dims[2]);
    int W_out = int(ctx.out_tensor.shape.dims[3]);
    int C_out = int(ctx.weight_tensor.shape.dims[0]);
    
    // Each workgroup handles one filter's bias gradient
    int f = int(gl_WorkGroupID.x);
    if (f >= C_out) return;
    
    int local_id = int(gl_LocalInvocationID.x) + int(gl_LocalInvocationID.y) * THREADS_X;
    int local_size = THREADS_X * THREADS_Y;
    
    float sum = 0.0;
    
    // Sum all grad_output values for this filter across all batches
    for (int b = 0; b < B; ++b) {
        int grad_out_offset = b * C_out * H_out * W_out + f * H_out * W_out;
        
        for (int idx = local_id; idx < H_out * W_out; idx += local_size) {
            sum += ctx.out_tensor.grad.grad[grad_out_offset + idx];
            ctx.out_tensor.grad.grad[grad_out_offset + idx] = 0.0f; // make sure to zero out the intermediate tensor's gradient at the end here
        }
    }
    
    // Reduce within workgroup using shared memory
    reduction[local_id] = sum;
    barrier();
    
    // Tree reduction
    for (int stride = local_size / 2; stride > 0; stride /= 2) {
        if (local_id < stride) {
            reduction[local_id] += reduction[local_id + stride];
        }
        barrier();
    }
    
    // Write result
    if (local_id == 0) {
        atomicAdd(ctx.bias_tensor.grad.grad[f], reduction[0]);
    }
}

void main() {
    if (push.context.ctx.kernel_type == 0) {
        compute_input_grad();
    } else if (push.context.ctx.kernel_type == 1) {
        compute_weight_grad();
    } else if (push.context.ctx.kernel_type == 2) {
        compute_bias_grad();
    }
}