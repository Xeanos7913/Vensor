#version 460
#extension GL_ARB_separate_shader_objects : enable
#extension GL_EXT_buffer_reference : enable
#extension GL_EXT_buffer_reference_uvec2 : enable
#extension GL_EXT_scalar_block_layout : enable
#extension GL_EXT_shader_atomic_float : enable
#extension GL_KHR_shader_subgroup_basic : enable
#extension GL_KHR_shader_subgroup_shuffle : enable
#extension GL_KHR_shader_subgroup_arithmetic : enable
#extension GL_KHR_shader_subgroup_shuffle_relative : enable

layout(local_size_x = 256, local_size_y = 1, local_size_z = 1) in;

#define MAX_WG 256
#define WARP_SIZE gl_SubgroupSize
#define VEC_TILE_SIZE 4  // Process 4 vec4s = 16 floats per thread

#define VECTORIZED_BATCHED // use vectorized MSE by default.

layout(buffer_reference, std430, scalar) buffer tensor_data {
    float data[];
};

layout(buffer_reference, std430, scalar) buffer tensor_grad {
    float grad[];
};

layout(buffer_reference, std430, scalar) buffer tensor_strides {
    int strides[];
};

layout(buffer_reference, std430, scalar) buffer tensor_shape {
    int dims[];
};

struct TensorImpl {
    tensor_data data;
    tensor_grad grad;
    tensor_strides strides;
    tensor_shape shape;
    uint numel, ndim, requires_grad, is_leaf;
};

struct MSE_context {
    TensorImpl target_tensor;   // [B, C, H, W] shape
    TensorImpl predicted_tensor;// [B, C, H, W] shape
    TensorImpl loss_tensor;     // [B] shape
    uint batch_size;            // B
    uint channels;              // C
    uint height;                // H
    uint width;                 // W
};

layout(buffer_reference, std430, scalar) buffer ContextBuffer {
    MSE_context ctx;
};

layout(push_constant) uniform PushConstants {
    ContextBuffer context;
    uvec3 grid_size;
    uint kernel_type;
} push;

// Shared memory for subgroup reduction
shared float shmem[MAX_WG];

// Optimized block-level reduction using subgroups
float blockReduceSum(float val) {
    uint tid = gl_LocalInvocationID.x;
    uint warp_id = tid / WARP_SIZE;
    uint lane_id = tid % WARP_SIZE;
    
    // Warp-level reduction using subgroup intrinsics
    val = subgroupAdd(val);
    
    // First thread in each warp writes to shared memory
    if (lane_id == 0) {
        shmem[warp_id] = val;
    }
    barrier();
    
    // First warp reduces the partial sums
    if (warp_id == 0) {
        val = (tid < (MAX_WG / WARP_SIZE)) ? shmem[lane_id] : 0.0;
        val = subgroupAdd(val);
    }
    
    return val;
}

void main() {
    MSE_context ctx = push.context.ctx;
    
    uint batchIdx = gl_WorkGroupID.z;  // Which batch element
    uint tid = gl_LocalInvocationID.x;
    
    if (batchIdx >= ctx.batch_size) return;
    
    // Calculate elements per batch: C * H * W
    uint elementsPerBatch = ctx.channels * ctx.height * ctx.width;
    uint batchOffset = batchIdx * elementsPerBatch;
    uint totalVec4s = elementsPerBatch / 4;
    
    float tileSquaredError = 0.0;
    float normalizer = (1.0 / float(elementsPerBatch));
    
    // Each thread processes VEC_TILE_SIZE vec4 elements (16 floats)
    uint baseVecIdx = tid * VEC_TILE_SIZE;
    
    #pragma unroll
    for (uint i = 0; i < VEC_TILE_SIZE; i++) {
        uint vecIdx = baseVecIdx + i;
        
        if (vecIdx < totalVec4s) {
            uint globalBaseIdx = batchOffset + (vecIdx * 4);
            
            // Vectorized load
            vec4 pred = vec4(
                ctx.predicted_tensor.data.data[globalBaseIdx],
                ctx.predicted_tensor.data.data[globalBaseIdx + 1],
                ctx.predicted_tensor.data.data[globalBaseIdx + 2],
                ctx.predicted_tensor.data.data[globalBaseIdx + 3]
            );
            
            vec4 targ = vec4(
                ctx.target_tensor.data.data[globalBaseIdx],
                ctx.target_tensor.data.data[globalBaseIdx + 1],
                ctx.target_tensor.data.data[globalBaseIdx + 2],
                ctx.target_tensor.data.data[globalBaseIdx + 3]
            );
            
            vec4 diff = pred - targ;
            vec4 squared = diff * diff;
            
            tileSquaredError += squared.x + squared.y + squared.z + squared.w;
            
            // Compute and write gradients: d(MSE)/d(pred) = 2 * (pred - targ) / N
            if (ctx.predicted_tensor.requires_grad != 0) {
                vec4 grad = 2.0 * diff * normalizer;
                
                ctx.predicted_tensor.grad.grad[globalBaseIdx] = grad.x;
                ctx.predicted_tensor.grad.grad[globalBaseIdx + 1] = grad.y;
                ctx.predicted_tensor.grad.grad[globalBaseIdx + 2] = grad.z;
                ctx.predicted_tensor.grad.grad[globalBaseIdx + 3] = grad.w;
            }
        }
    }
    
    // Handle remaining elements (if elementsPerBatch % 4 != 0)
    uint remainingStart = totalVec4s * 4;
    for (uint localIdx = remainingStart + tid; localIdx < elementsPerBatch; localIdx += MAX_WG) {
        uint globalIdx = batchOffset + localIdx;
        
        float pred = ctx.predicted_tensor.data.data[globalIdx];
        float targ = ctx.target_tensor.data.data[globalIdx];
        float diff = pred - targ;
        
        tileSquaredError += diff * diff;
        
        if (ctx.predicted_tensor.requires_grad != 0) {
            ctx.predicted_tensor.grad.grad[globalIdx] = 2.0 * diff * normalizer;
        }
    }
    
    // Reduce within workgroup to get batch loss
    float batchLoss = blockReduceSum(tileSquaredError);
    
    // First thread writes the per-batch loss (mean over batch elements)
    if (tid == 0) {
        ctx.loss_tensor.data.data[batchIdx] = batchLoss * normalizer;
    }
}
