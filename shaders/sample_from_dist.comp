#version 450
#extension GL_ARB_separate_shader_objects : enable
#extension GL_EXT_buffer_reference : enable
#extension GL_EXT_buffer_reference_uvec2 : enable
#extension GL_EXT_scalar_block_layout : enable

// Workgroup size (runtime specialization constants)
layout(local_size_x = 256, local_size_y = 1, local_size_z = 1) in;

layout(buffer_reference, std430, buffer_reference_align = 16) buffer tensor_data {
    float data[];
};

layout(buffer_reference, std430, buffer_reference_align = 16) buffer tensor_grad {
    float grad[];
};

layout(buffer_reference, std430, buffer_reference_align = 16) buffer tensor_strides {
    int strides[];
};

layout(buffer_reference, std430, buffer_reference_align = 16) buffer tensor_shape {
    int dims[];
};

struct TensorImpl {
    tensor_data    data;
    tensor_grad    grad;
    tensor_strides strides;
    tensor_shape   shape;
    uint numel;
    uint ndim;
    uint requires_grad;
    uint is_leaf;
};

struct SampleContext {
    TensorImpl input_tensor;  // going to be a vector tensor shaped (B, 1, N) [the prob distribution]
    TensorImpl output_tensor; // output tensor (B)
    uint M, N; // for 2D, M = rows, N = cols; for 3D, M = first dim, N = second dim
};

layout(buffer_reference, std430, buffer_reference_align = 16) buffer ContextBuffer {
    SampleContext ctx;
};

layout(push_constant) uniform PushConstants{
    ContextBuffer ctx;
    uvec3 grid_size;
    uint mode; // 0 = forward; 1 = backward;
} push;

// Shared memory for tiled computation
shared float shared_data[256];
shared float shared_max[256];

uint off2or3(const TensorImpl t, uint b, uint i, uint j)
{
    if (t.ndim == 2u) {
        return i * t.strides.strides[0] + j * t.strides.strides[1];
    } else {
        return b * t.strides.strides[0] + i * t.strides.strides[1] + j * t.strides.strides[2];
    }
}

void main() {
    SampleContext ctx = push.ctx.ctx;
    
    uint b = gl_WorkGroupID.z;  // Batch index
    uint global_id = gl_WorkGroupID.x * gl_WorkGroupSize.x + gl_LocalInvocationID.x;
    uint tid = gl_LocalInvocationID.x;
    uint N = ctx.N;
    
    // Phase 1: Each thread finds its local maximum
    float my_max = -1e30;
    uint my_idx = 0;
    
    // Process multiple elements per thread if N > total threads
    for (uint i = global_id; i < N; i += gl_NumWorkGroups.x * gl_WorkGroupSize.x) {
        uint offset = off2or3(ctx.input_tensor, b, 0, i);
        float val = ctx.input_tensor.data.data[offset];
        if (val > my_max) {
            my_max = val;
            my_idx = i;
        }
    }
    
    // Phase 2: Workgroup reduction to find max within this workgroup
    shared_max[tid] = my_max;
    shared_data[tid] = float(my_idx);  // Store index as float
    barrier();
    
    for (uint stride = 128; stride > 0; stride >>= 1) {
        if (tid < stride && tid + stride < gl_WorkGroupSize.x) {
            if (shared_max[tid + stride] > shared_max[tid]) {
                shared_max[tid] = shared_max[tid + stride];
                shared_data[tid] = shared_data[tid + stride];
            }
        }
        barrier();
    }
    
    // Phase 3: Write workgroup result
    // Note: If there are multiple workgroups per batch, you need atomic operations
    // or a two-pass approach. For simplicity, assuming single workgroup per batch:
    if (tid == 0) {
        uint max_idx = uint(shared_data[0]);
        
        if (ctx.output_tensor.ndim == 1) {
            ctx.output_tensor.data.data[b] = float(max_idx);
        } else {
            uint out_offset = off2or3(ctx.output_tensor, b, 0, 0);
            ctx.output_tensor.data.data[out_offset] = float(max_idx);
        }
    }
}